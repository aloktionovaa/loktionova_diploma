############################### интерфакс ###########################
####объединить все файлы####
library(reshape)
setwd("/Users/macbook/Desktop/interfax")
#считываем сразу все файлы из папки
uniq <- function(x) {
  x <- x[!duplicated(x[,c(1:2,5)]),]
}
file.list <- list.files(pattern='*.xlsx')
library(readxl)
df.list <- lapply(file.list, as.data.frame(read_excel))
df.list <- lapply(df.list, function(x) dplyr::select(x, -1))
df.list <- lapply(df.list, uniq)
#мэтчим все файлы
all_interfax <- merge_recurse(df.list)
View(all_interfax)
#изменяем все имена
names(all_interfax) <- gsub("value.", "", names(all_interfax))
library(writexl)
write_xlsx(all_interfax, 'all_interfax_2021_2023.xlsx')
#хотим поменять тип сразу нескольких переменных
#library(magrittr)
#cols = seq(2, 52, by=2)
#all20[,cols] %<>% lapply(function(x) as.numeric(x))

##### посмотрим что получилось ####
library(readxl)
all_interfax_2021_2023 <- read_excel("all_interfax_2021_2023.xlsx")
View(all_interfax_2021_2023)
data2 <- all_interfax_2021_2023
data2$Дата <- as.Date(data2$Дата, tryFormats = c("%d-%m-%Y"))
?as.Date
View(data2)

#разделим на токены и удалим стоп-слова
data_tokens <- tokens(
  data2$Статьи, what = "word", remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
  remove_url = TRUE, remove_separators = TRUE, split_hyphens = FALSE, split_tags = FALSE,
  include_docvars = TRUE, padding = FALSE, verbose = quanteda_options("verbose")) %>%
  tokens_remove(pattern = quanteda::stopwords("ru", source = "snowball")) %>%
  tokens_remove(pattern = quanteda::stopwords("ru", source = "stopwords-iso")) %>% 
  tokens_remove(pattern = quanteda::stopwords("ru", source = "marimo")) %>%
  tokens_remove(pattern = quanteda::stopwords("ru", source = "nltk")) %>%
  tokens_tolower()

#удалим пунктуацию и цифры и превратим в DFM
data_tokens <- tokens_remove(data_tokens, pattern = c(stopwords("ru"), "*-time", "updated-*", 
                                                                    "gmt", "bst"))

data_tokens <- tokens_wordstem(data_tokens, language = "ru") 


dfmat_data_tokens <- dfm(data_tokens) %>% 
  dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile",
           max_docfreq = 0.1, docfreq_type = "prop")

#определим количество словарей
library("ldatuning")
library("topicmodels")

raw.sum=apply(dfmat_data_tokens,1,FUN=sum) #sum by raw each raw of the table
dfmat_data_tokens=dfmat_data_tokens[raw.sum!=0,]

#работает 8-10 часов 
result <- FindTopicsNumber(
  dfmat_data_tokens,
  topics = seq(from = 20, to = 100, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)
FindTopicsNumber_plot(result)


#латентное размещение Дирихле
library(seededlda)
library(lubridate)
set.seed(1234)
lda_interfax_40 <- textmodel_lda(dfmat_data_tokens, k = 40)
lda_interfax_60 <- textmodel_lda(dfmat_data_tokens, k = 60)
ld60 <- terms(lda_interfax_60, 22)

ld <- terms(lda_interfax_40, 22)
View(ld)
ld <- as.data.frame(ld)

??svglite

write_xlsx(as.data.frame(ld60), 'interfax_topics_60.xlsx')

#по топикам
interfax_topics <- lda_interfax_60$theta
interfax_topics1 <- as.data.frame(interfax_topics)
View(interfax_topics1)
dim(interfax_topics1)
write_xlsx(as.data.frame(interfax_topics1), 'interfax_60.xlsx')

#Прицепили это к изначальному датасету - теперь есть пара новость и то, вероятности по топикам 
itog_interfax_topics1 <- cbind(data2, raw.sum)
#View(itog_interfax_topics1)

itog_interfax_topics1 <- itog_interfax_topics1 %>% filter(itog_interfax_topics1$raw.sum !='0')

dim(interfax_topics1)
itog_interfax_topics2 <- cbind(itog_interfax_topics1, interfax_topics)
itog_interfax_topics2 <- itog_interfax_topics2[!duplicated(itog_interfax_topics2$Статьи),]
#49851 новостей

#### Загрузим доходности ####
library(reshape)   
library(readr)
library(readxl)
#обратимся к папке и достанем оттуда файлы 
setwd("/Users/macbook/Desktop/Доходность")
file.list <- list.files(pattern='*.xlsx')
df.list <- lapply(file.list, as.data.frame(read_excel))
View(df.list)

f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data11 <- NULL
data13 <- NULL
library(lubridate)
library(dplyr)
library(glmnet)
library(solzy)
library(caret)
library(MASS)
library(randomForest)


#### сделаем 7 лагов топиков #### 
df_int <- df_int
interfax_df_int <- df_int
interfax_df_int_base <- df_int[,1:31]
View(df10)
View(interfax_df_int_base)

#сохраним на всякий
write_xlsx(df_int, 'interfax_topics_30_с_лагами.xlsx')

#### Загрузим доходность с топиками ####
library(reshape)   
library(readr)
library(readxl)
#обратимся к папке и достанем оттуда файлы 
setwd("/Users/macbook/Desktop/Доходность")
file.list <- list.files(pattern='*.xlsx')
df.list <- lapply(file.list, as.data.frame(read_excel))
View(df.list)
library(lubridate)
library(dplyr)
library(glmnet)
library(solzy)
library(caret)
library(MASS)
library(randomForest)

f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data11 <- NULL
data13 <- NULL

#цикл для AR процесса
for(i in 1:200) {
  cat("Итерация номер", i, "\n")
  #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,c(1:11)]
  #изменяем все имена
  names(data_comp) <- gsub("value.", "", names(data_comp))
  
  #отфильтруем даты
  data_comp <- data_comp[order(as.Date(data_comp$data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$data))
  data_comp$data <- as.Date(data_comp$data)
  names(interfax_df_int_base)[1] <- 'data'
  
  #объединим словари и доходности
  data_comp <- left_join(interfax_df_int_base,data_comp, by = 'data')
  
  #расставим 0 и 1
  data_comp$ind <- -(is.na(data_comp$price_close)*1 - 1)
  
  #найдем двойные пропуски
  data_comp$ind2 <- 0
  for(j in 2:(dim(data_comp)[1]-1)) {
    data_comp$ind2[j] <- (data_comp$ind[j] == 0 & data_comp$ind[j-1]*data_comp$ind[j+1] == 0)*1
  }
  
  #заполним средним
  for(l in 2:31){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(l_for_names[1,1]-1)
    #слово топик для названия
    b <- 'mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  #data_comp$d_mean <- data_comp$d
  names(data_comp)
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 44:73) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  data_comp <- data_comp[,-c(2:31, 42,43)]
  
  #сделаем лаги топиков
  
  for(s in 12:41) {
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    s_for_names <- as.data.frame(s)
    a <- as.character(s_for_names[1,1]-11)
    #слово топик для названия
    b <- 'mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],1)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
    
    #сделаем второй лаг и название 
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],2)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
    
    #и так далее
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],3)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],4)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],5)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],6)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],7)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
  }
  names(data_comp)
  data_comp <- na.omit(data_comp)
  
  data_comp[-1,(dim(data_comp)[2]+1)] <- diff(as.numeric(data_comp[,3]))/na.omit(dplyr::lag(as.numeric(data_comp$price_close), 1))
  #лаги доходности
  data_comp$y_lag_1 <- lag(data_comp[,252], 1)
  data_comp$y_lag_2 <- lag(data_comp[,252], 2)
  data_comp$y_lag_3 <- lag(data_comp[,252], 3)
  data_comp$y_lag_4 <- lag(data_comp[,252], 4)
  data_comp$y_lag_5 <- lag(data_comp[,252], 5)
  data_comp$y_lag_6 <- lag(data_comp[,252], 6)
  data_comp$y_lag_7 <- lag(data_comp[,252], 7)
  
  #удалили лишнее и сделали лаги
  #data2 <- select(data, -ind, -ind2, - d)
  #data2
  
  #удалим месяц когда не работала биржа и еще 7 дней
  
  #установка диапазона дат
  start_date <- as.Date("2022-02-25")
  end_date <- as.Date("2022-04-03")
  
  # Удаление строк с датами в этом диапазоне
  data_comp <- data_comp[!(data_comp$data >= start_date & data_comp$data <= end_date),]
  data_comp <- na.omit(data_comp)
  names(data_comp)
  data1 <- data_comp[,c(12:251)]
  
  #### удалим аномальные доходности####
  N <- which(abs(data_comp[,252])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  f <- c(f,length(N))
  
  #лассо
  mod <- glmnet(data1,data_comp[,252], 
                alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path <- as.data.frame(glmnetPath(mod)$leave)
  a <- max(glmnetPath(mod)$leave$ord)-2
  b <- max(glmnetPath(mod)$leave$ord)
  vec <- as.data.frame(a:b)
  names(vec)[1] <- 'ord'
  
  #отберем значимые топики
  path <- left_join(vec,path,by='ord')
  top <- c(path$var)
  
  #отберем топики
  reg <- data1 %>% dplyr::select(top)
  
  data_y_lags <- as.data.frame(data_comp[,c(252:259)])
  names(data_y_lags)[1] <- 'yield'
  
  #по лагам
  set.seed(789)
  mod1 <- train(yield~.,
                data=data_y_lags, method = "lm",
                trControl = trainControl(method = 'repeatedcv',
                                         number = 10, repeats = 10))
  
  #достанем значимые лаги
  p_val <- as.data.frame(summary(mod1)$coefficients[-1,4]) 
  names(p_val)[1] <- 'pvalue'
  p_val <- p_val %>% filter(pvalue<0.1)
  lags <- c(rownames(p_val))
  lags <- data_comp %>% dplyr::select(lags)
  dim(lags)
  dim(data_comp)
  dim(reg)
  #сделаем датасет со значимым лагом доходности и значимыми топиками
  all <- as.data.frame(cbind(data_comp[,252], lags,reg))
  names(all)[1] <- 'yield'
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  mod2 <- caret::train(yield ~ .,data=all, 
                       method = "lm",
                       trControl = trainControl(method = 'repeatedcv',
                                                number = 10, repeats = 10))
  summary(mod2)
  
  #достанем значимые лаги
  table_full_AR <- as.data.frame(summary(mod2)$coefficients)
  names(table_full_AR)[4] <- 'P-value'
  g <- grep('topic_', rownames(table_full_AR))
  table <- as.data.frame(table_full_AR[g,])
  table_full_AR <- table %>% dplyr::filter(table[,4] < 0.1)
  table_full_AR_2 <- as.data.frame(rownames(table_full_AR))
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR else cbind(table_full_AR_2,table_full_AR)
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR_1 else table_full_AR_1 %>% mutate(ticker=list_comp[i])
  data11 <- rbind(data11,table_full_AR_1)
  t <- t.test(mod1$resample$Rsquared,mod2$resample$Rsquared)
  
  r2_no_news <- mean(mod1$resample$Rsquared)
  r2_with_news <- mean(mod2$resample$Rsquared)
  delta1 <- r2_with_news - r2_no_news
  
  adj_r2_no_news <- summary(mod1)$adj.r.squared
  adj_r2_with_news <- summary(mod2)$adj.r.squared
  delta2 <- adj_r2_with_news - adj_r2_no_news
  
  data13 <- rbind(data13,c(list_comp[i], t$statistic, 
                           r2_no_news, r2_with_news,
                           delta1,
                           adj_r2_no_news,adj_r2_with_news,
                           delta2))
}

data13 <- as.data.frame(data13)
names(data13)
data13$t <- as.numeric(data13$t)
df15 <- data13 %>% filter(data13$t < -1.64)

View(df15)
#150 компаний
data11 <- as.data.frame(data11)
df2 <- filter(data11, data11$ticker %in% df15$V1)
View(df2)

write_xlsx(df2,'значимые_топики_интерфакс_7_дней.xlsx')
write_xlsx(data13,'интерфакс_10_10rep_7_lags.xlsx')
#127 компаний


prop.test(x = c(117, 127), n = c(200, 200))

################### NYT ########################
library(readxl)
nyt_2021_2023 <- read_excel("/Users/macbook/Desktop/NYT_full.xlsx")
View(nyt_2021_2023)
data5 <- nyt_2021_2023

#### предобработка текста ####
data5_tokens <- tokens(
  data5$full_text_news, what = "word", remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
  remove_url = TRUE, remove_separators = TRUE, split_hyphens = FALSE, split_tags = FALSE,
  include_docvars = TRUE, padding = FALSE, verbose = quanteda_options("verbose")) %>%
  tokens_remove(pattern = quanteda::stopwords("en", source = "snowball")) %>%
  tokens_remove(pattern = quanteda::stopwords("en", source = "stopwords-iso")) %>% 
  tokens_remove(pattern = quanteda::stopwords("en", source = "marimo")) %>%
  tokens_remove(pattern = quanteda::stopwords("en", source = "nltk")) %>%
  tokens_tolower()

#удалим пунктуацию и цифры и превратим в DFM
data5_tokens <- tokens_remove(data5_tokens, pattern = c(stopwords("en"), "*-time", "updated-*", 
                                                                    "gmt", "bst"))

data5_tokens <- tokens_wordstem(data5_tokens, language = "en") 


dfmat_final_data5_tokens <- dfm(data5_tokens) %>% 
  dfm_trim(min_termfreq = 0.8, termfreq_type = "quantile",
           max_docfreq = 0.3, docfreq_type = "prop")

raw.sum1=apply(dfmat_final_data5_tokens,1,FUN=sum) #sum by raw each raw of the table
dfmat_final_data5_tokens =dfmat_final_data5_tokens[raw.sum1!=0,]

#### модель LDA ####
#определим количество словарей
library("ldatuning")
library("topicmodels")

#работает 8-10 часов 
result <- FindTopicsNumber(
  dfmat_final_itog2_tokens,
  topics = seq(from = 2, to = 25, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)

#латентное размещение Дирихле
library(seededlda)
library(lubridate)
set.seed(1234)
lda_nyt_30 <- textmodel_lda(dfmat_final_data5_tokens, k = 30)

ld1 <- terms(lda_nyt_30, 22)
View(ld1)
ld1 <- as.data.frame(ld1)

#### достанем словари ####
#по топикам
nyt_topics <- lda_nyt_30$theta
nyt_topics1 <- as.data.frame(nyt_topics)
View(nyt_topics1)
dim(nyt_topics1)
#write_xlsx(as.data.frame(interfax_topics1), 'interfax_60.xlsx')

#Прицепили это к изначальному датасету - теперь есть пара новость и то, вероятности по топикам 
itog_nyt_topics1 <- cbind(data5, raw.sum1)
#View(itog_interfax_topics1)

itog_nyt_topics1 <- itog_nyt_topics1 %>% filter(itog_nyt_topics1$raw.sum1 !='0')

dim(itog_nyt_topics1)

itog_itog_nyt_topics2 <- cbind(itog_nyt_topics1, nyt_topics1)
itog_itog_nyt_topics2  <- itog_itog_nyt_topics2[!duplicated(itog_itog_nyt_topics2$full_text_news),]
#3,983
head(itog_itog_nyt_topics2)

### среднее по дням ###
nyt_mean_topics <- itog_itog_nyt_topics2 %>% group_by(pub_date) %>% 
  summarise(topic_1 = mean(topic1), 
            topic_2 = mean(topic2), 
            topic_3 = mean(topic3), 
            topic_4 = mean(topic4),
            topic_5 = mean(topic5), 
            topic_6 = mean(topic6), 
            topic_7 = mean(topic7), 
            topic_8 = mean(topic8),
            topic_9 = mean(topic9), 
            topic_10 = mean(topic10),
            topic_11 = mean(topic11), 
            topic_12 = mean(topic12), 
            topic_13 = mean(topic13), 
            topic_14 = mean(topic14),
            topic_15 = mean(topic15), 
            topic_16 = mean(topic16), 
            topic_17 = mean(topic17), 
            topic_18 = mean(topic18),
            topic_19 = mean(topic19), 
            topic_20 = mean(topic20),
            topic_21 = mean(topic21),
            topic_22 = mean(topic22),
            topic_23 = mean(topic23),
            topic_24 = mean(topic24),
            topic_25 = mean(topic25),
            topic_26 = mean(topic26), 
            topic_27 = mean(topic27), 
            topic_28 = mean(topic28),
            topic_29 = mean(topic29),
            topic_30 = mean(topic30),
            .groups = 'drop') %>%
  as.data.frame()

#View(nyt_mean_topics)

#### нужно сделать датасет с пустыми датами ####
library(tidyverse)
library(lubridate)

#пустой датасет с последовательностью дат
date_df <- seq.Date(from = as.Date('2021-09-01'),
                    to = as.Date('2023-08-31'),
                    by = "day") %>% 
  as.data.frame(col.names = c("pub_date"))
names(date_df)[1] <- "pub_date"

#уникальные даты из датасета нашего
df2 <- itog_itog_nyt_topics2
df3 <- as.data.frame(df2$pub_date)
df3 <- df3[!duplicated(df3$`df2$pub_date`),]
df3 <- as.data.frame(df3)
View(df3)
names(df3)[1] <- 'pub_date'
#определим те даты, которых нет в датасете с новостями
df5 <- anti_join(date_df,df3)
View(df5)
head(df5)
m <- matrix(0, ncol = 30, nrow = 47)
df6 <- data.frame(m)

df5 <- cbind(df5,df6)
View(df5)

colnames(df5)[2:31] <- c('topic_1','topic_2','topic_3','topic_4','topic_5', 'topic_6', 
                'topic_7','topic_8', 'topic_9','topic_10','topic_11', 'topic_12', 
                'topic_13','topic_14','topic_15', 'topic_16', 'topic_17', 'topic_18',
                'topic_19', 'topic_20','topic_21','topic_22','topic_23','topic_24', 
                'topic_25','topic_26', 'topic_27', 'topic_28','topic_29','topic_30')

#### итоговый датасет со всеми датами и словарями ####
df8 <- rbind(nyt_mean_topics, df5)
View(df8)
#отфильтруем даты
df9 <- df8[order(as.Date(df8$pub_date, format="%Y%m/%d/")),]
View(df9)
df9 <- df9 %>% dplyr::arrange(mdy(df9$pub_date))
df9$pub_date <- as.Date(df9$pub_date)


#### сделаем 14 лагов топиков #### 
df10 <- df9

for(i in 2:31) {
  #зададим исходную размерность
dim_1 <- as.data.frame(dim(df10))
#достанем номер топика
i_for_names <- as.data.frame(i)
a <- as.character(i_for_names[1,1]-1)
#слово топик для названия
b <- 'topic'

#сделаем первый лаг и введем название 
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],1)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')

#сделаем второй лаг и название 
dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],2)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')

#и так далее
dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],3)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')

dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],4)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')

dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],5)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')

dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],6)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')

dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],7)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')

dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],8)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'8', sep='_')

dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],9)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'9', sep='_')

dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],10)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'10', sep='_')

dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],11)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'11', sep='_')

dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],12)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'12', sep='_')

dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],13)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'13', sep='_')

dim_1 <- as.data.frame(dim(df10))
df10[,(dim_1[2,1]+1)] <- lag(df10[,i],14)
names(df10)[dim_1[2,1]+1] <- paste(b,a,'14', sep='_')
}

View(df10)

#сохраним на всякий
write_xlsx(df10, 'nyt_topics_30_с_лагами.xlsx')

#### Загрузим доходность с топиками ####
library(reshape)   
library(readr)
library(readxl)
#обратимся к папке и достанем оттуда файлы 
setwd("/Users/macbook/Desktop/Доходность с топиками телеграм")
file.list <- list.files(pattern='*.xlsx')
df.list <- lapply(file.list, as.data.frame(read_excel))
View(df.list)

f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data11 <- NULL
data13 <- NULL
library(lubridate)

library(dplyr)
library(glmnet)
library(solzy)
library(caret)
library(MASS)
library(randomForest)

#цикл для AR процесса
for(i in 1:200) {
  cat("Итерация номер", i, "\n")
  #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,c(1:11)]
  #изменяем все имена
  names(data_comp) <- gsub("value.", "", names(data_comp))
  
  #отфильтруем даты
  data_comp <- data_comp[order(as.Date(data_comp$data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$data))
  data_comp$data <- as.Date(data_comp$data)
  names(df10)[1] <- 'data'
  data_comp <- left_join(data_comp,df10, by = 'data')
  
  data_comp[,c(2:5, 12:461)] <- data_comp[,c(2:5, 12:461)] %>% mutate_all(as.numeric)
  data_comp[-1,462] <- diff(data_comp[,2])/na.omit(dplyr::lag(data_comp$price, 1))
  
  data1 <- data_comp[,c(12:461)]

  #### удалим аномальные доходности####
  N <- which(abs(data_comp[,462])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  f <- c(f,length(N))
  
  #лассо
  mod <- glmnet(data1[-c(1:15),],data_comp[-c(1:15),462], 
                alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path <- as.data.frame(glmnetPath(mod)$leave)
  a <- max(glmnetPath(mod)$leave$ord)-2
  b <- max(glmnetPath(mod)$leave$ord)
  vec <- as.data.frame(a:b)
  names(vec)[1] <- 'ord'
  
  #отберем значимые топики
  path <- left_join(vec,path,by='ord')
  top <- c(path$var)
  
  #отберем топики
  reg <- data1[-c(1:15),] %>% dplyr::select(top)
  
  #лаги доходности
  data_comp$y_lag_1 <- lag(data_comp[,462], 1)
  data_comp$y_lag_2 <- lag(data_comp[,462], 2)
  data_comp$y_lag_3 <- lag(data_comp[,462], 3)
  data_comp$y_lag_4 <- lag(data_comp[,462], 4)
  data_comp$y_lag_5 <- lag(data_comp[,462], 5)
  data_comp$y_lag_6 <- lag(data_comp[,462], 6)
  data_comp$y_lag_7 <- lag(data_comp[,462], 7)
  data_comp$y_lag_8 <- lag(data_comp[,462], 8)
  data_comp$y_lag_9 <- lag(data_comp[,462], 9)
  data_comp$y_lag_10 <- lag(data_comp[,462], 10)
  data_comp$y_lag_11 <- lag(data_comp[,462], 11)
  data_comp$y_lag_12 <- lag(data_comp[,462], 12)
  data_comp$y_lag_13 <- lag(data_comp[,462], 13)
  data_comp$y_lag_14 <- lag(data_comp[,462], 14)
  dim(data_comp)

  data_y_lags <- as.data.frame(data_comp[-c(1:15),c(462:476)])
  names(data_y_lags)[1] <- 'yield'
  
  #по лагам
  set.seed(789)
  mod1 <- train(yield~.,
                data=data_y_lags, method = "lm",
                trControl = trainControl(method = 'repeatedcv',
                                         number = 10, repeats = 10))
  
  #достанем значимые лаги
  p_val <- as.data.frame(summary(mod1)$coefficients[-1,4]) 
  names(p_val)[1] <- 'pvalue'
  p_val <- p_val %>% filter(pvalue<0.1)
  lags <- c(rownames(p_val))
  lags <- data_comp %>% dplyr::select(lags)
  
  #сделаем датасет со значимым лагом доходности и значимыми топиками
  all <- as.data.frame(cbind(data_comp[-c(1:15),462], lags[-c(1:15),],reg))
  names(all)[1] <- 'yield'
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  mod2 <- caret::train(yield ~ .,data=all, 
                       method = "lm",
                       trControl = trainControl(method = 'repeatedcv',
                                                number = 10, repeats = 10))
  summary(mod2)
  
  #достанем значимые лаги
  table_full_AR <- as.data.frame(summary(mod2)$coefficients)
  names(table_full_AR)[4] <- 'P-value'
  g <- grep('topic_', rownames(table_full_AR))
  table <- as.data.frame(table_full_AR[g,])
  table_full_AR <- table %>% dplyr::filter(table[,4] < 0.1)
  table_full_AR_2 <- as.data.frame(rownames(table_full_AR))
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR else cbind(table_full_AR_2,table_full_AR)
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR_1 else table_full_AR_1 %>% mutate(ticker=list_comp[i])
  data11 <- rbind(data11,table_full_AR_1)
  t <- t.test(mod1$resample$Rsquared,mod2$resample$Rsquared)
  
  r2_no_news <- mean(mod1$resample$Rsquared)
  r2_with_news <- mean(mod2$resample$Rsquared)
  delta1 <- r2_with_news - r2_no_news
  
  adj_r2_no_news <- summary(mod1)$adj.r.squared
  adj_r2_with_news <- summary(mod2)$adj.r.squared
  delta2 <- adj_r2_with_news - adj_r2_no_news
  
  data13 <- rbind(data13,c(list_comp[i], t$statistic, 
                           r2_no_news, r2_with_news,
                           delta1,
                           adj_r2_no_news,adj_r2_with_news,
                           delta2))
}

data13 <- as.data.frame(data13)
names(data13)
data13$t <- as.numeric(data13$t)
df15 <- data13 %>% filter(data13$t < -1.64)
View(df15)

data11 <- as.data.frame(data11)
write_xlsx(data11,'topics_nyt_10_10rep_14_lags.xlsx')
write_xlsx(data13,'nyt_10_10rep_14_lags.xlsx')

#### сделаем 7 лагов топиков #### 
df10 <- df9
df10 <- df10[,1:31]

for(i in 2:31) {
  #зададим исходную размерность
  dim_1 <- as.data.frame(dim(df10))
  #достанем номер топика
  i_for_names <- as.data.frame(i)
  a <- as.character(i_for_names[1,1]-1)
  #слово топик для названия
  b <- 'topic'
  
  #сделаем первый лаг и введем название 
  df10[,(dim_1[2,1]+1)] <- lag(df10[,i],1)
  names(df10)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
  
  #сделаем второй лаг и название 
  dim_1 <- as.data.frame(dim(df10))
  df10[,(dim_1[2,1]+1)] <- lag(df10[,i],2)
  names(df10)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
  
  #и так далее
  dim_1 <- as.data.frame(dim(df10))
  df10[,(dim_1[2,1]+1)] <- lag(df10[,i],3)
  names(df10)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
  
  dim_1 <- as.data.frame(dim(df10))
  df10[,(dim_1[2,1]+1)] <- lag(df10[,i],4)
  names(df10)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
  
  dim_1 <- as.data.frame(dim(df10))
  df10[,(dim_1[2,1]+1)] <- lag(df10[,i],5)
  names(df10)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
  
  dim_1 <- as.data.frame(dim(df10))
  df10[,(dim_1[2,1]+1)] <- lag(df10[,i],6)
  names(df10)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
  
  dim_1 <- as.data.frame(dim(df10))
  df10[,(dim_1[2,1]+1)] <- lag(df10[,i],7)
  names(df10)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
}

View(df10)

#сохраним на всякий
#write_xlsx(df10, 'nyt_topics_30_с_лагами.xlsx')

#### Загрузим доходность с топиками ####
library(reshape)   
library(readr)
library(readxl)
#обратимся к папке и достанем оттуда файлы 
setwd("/Users/macbook/Desktop/Доходность с топиками телеграм")
file.list <- list.files(pattern='*.xlsx')
df.list <- lapply(file.list, as.data.frame(read_excel))
View(df.list)

#### модель ####
f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data11 <- NULL
data13 <- NULL

#цикл для AR процесса
for(i in 1:200) {
  cat("Итерация номер", i, "\n")
  #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,c(1:11)]
  #изменяем все имена
  names(data_comp) <- gsub("value.", "", names(data_comp))
  
  #отфильтруем даты
  data_comp <- data_comp[order(as.Date(data_comp$data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$data))
  data_comp$data <- as.Date(data_comp$data)
  names(df10)[1] <- 'data'
  
  #объединим словари и доходности
  data_comp <- left_join(df10,data_comp, by = 'data')
  
  #расставим 0 и 1
  data_comp$ind <- -(is.na(data_comp$price_close)*1 - 1)
  
  #найдем двойные пропуски
  data_comp$ind2 <- 0
  for(j in 2:(dim(data_comp)[1]-1)) {
    data_comp$ind2[j] <- (data_comp$ind[j] == 0 & data_comp$ind[j-1]*data_comp$ind[j+1] == 0)*1
  }
  
  #заполним средним
  for(l in 2:31){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(l_for_names[1,1]-1)
    #слово топик для названия
    b <- 'mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  #data_comp$d_mean <- data_comp$d
  names(data_comp)
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 44:73) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  data_comp <- data_comp[,-c(2:31, 42,43)]
  
  #сделаем лаги топиков
  
  for(s in 12:41) {
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    s_for_names <- as.data.frame(s)
    a <- as.character(s_for_names[1,1]-11)
    #слово топик для названия
    b <- 'mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],1)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
    
    #сделаем второй лаг и название 
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],2)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
    
    #и так далее
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],3)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],4)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],5)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],6)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],7)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
  }
  names(data_comp)
  data_comp <- na.omit(data_comp)
  
  data_comp[-1,(dim(data_comp)[2]+1)] <- diff(as.numeric(data_comp[,3]))/na.omit(dplyr::lag(as.numeric(data_comp$price_close), 1))
  #лаги доходности
  data_comp$y_lag_1 <- lag(data_comp[,252], 1)
  data_comp$y_lag_2 <- lag(data_comp[,252], 2)
  data_comp$y_lag_3 <- lag(data_comp[,252], 3)
  data_comp$y_lag_4 <- lag(data_comp[,252], 4)
  data_comp$y_lag_5 <- lag(data_comp[,252], 5)
  data_comp$y_lag_6 <- lag(data_comp[,252], 6)
  data_comp$y_lag_7 <- lag(data_comp[,252], 7)
  
  #удалили лишнее и сделали лаги
  #data2 <- select(data, -ind, -ind2, - d)
  #data2
  
  #удалим месяц когда не работала биржа и еще 7 дней
  
  #установка диапазона дат
  start_date <- as.Date("2022-02-25")
  end_date <- as.Date("2022-04-03")
  
  # Удаление строк с датами в этом диапазоне
  data_comp <- data_comp[!(data_comp$data >= start_date & data_comp$data <= end_date),]
  data_comp <- na.omit(data_comp)
  names(data_comp)
  data1 <- data_comp[,c(12:251)]
  
  #### удалим аномальные доходности####
  N <- which(abs(data_comp[,252])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  f <- c(f,length(N))
  
  #лассо
  mod <- glmnet(data1,data_comp[,252], 
                alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path <- as.data.frame(glmnetPath(mod)$leave)
  a <- max(glmnetPath(mod)$leave$ord)-2
  b <- max(glmnetPath(mod)$leave$ord)
  vec <- as.data.frame(a:b)
  names(vec)[1] <- 'ord'
  
  #отберем значимые топики
  path <- left_join(vec,path,by='ord')
  top <- c(path$var)
  
  #отберем топики
  reg <- data1 %>% dplyr::select(top)
  
  data_y_lags <- as.data.frame(data_comp[,c(252:259)])
  names(data_y_lags)[1] <- 'yield'
  
  #по лагам
  set.seed(789)
  mod1 <- train(yield~.,
                data=data_y_lags, method = "lm",
                trControl = trainControl(method = 'repeatedcv',
                                         number = 10, repeats = 10))
  
  #достанем значимые лаги
  p_val <- as.data.frame(summary(mod1)$coefficients[-1,4]) 
  names(p_val)[1] <- 'pvalue'
  p_val <- p_val %>% filter(pvalue<0.1)
  lags <- c(rownames(p_val))
  lags <- data_comp %>% dplyr::select(lags)
  dim(lags)
  dim(data_comp)
  dim(reg)
  #сделаем датасет со значимым лагом доходности и значимыми топиками
  all <- as.data.frame(cbind(data_comp[,252], lags,reg))
  names(all)[1] <- 'yield'
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  mod2 <- caret::train(yield ~ .,data=all, 
                       method = "lm",
                       trControl = trainControl(method = 'repeatedcv',
                                                number = 10, repeats = 10))
  summary(mod2)
  
  #достанем значимые лаги
  table_full_AR <- as.data.frame(summary(mod2)$coefficients)
  names(table_full_AR)[4] <- 'P-value'
  g <- grep('topic_', rownames(table_full_AR))
  table <- as.data.frame(table_full_AR[g,])
  table_full_AR <- table %>% dplyr::filter(table[,4] < 0.1)
  table_full_AR_2 <- as.data.frame(rownames(table_full_AR))
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR else cbind(table_full_AR_2,table_full_AR)
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR_1 else table_full_AR_1 %>% mutate(ticker=list_comp[i])
  data11 <- rbind(data11,table_full_AR_1)
  t <- t.test(mod1$resample$Rsquared,mod2$resample$Rsquared)
  
  r2_no_news <- mean(mod1$resample$Rsquared)
  r2_with_news <- mean(mod2$resample$Rsquared)
  delta1 <- r2_with_news - r2_no_news
  
  adj_r2_no_news <- summary(mod1)$adj.r.squared
  adj_r2_with_news <- summary(mod2)$adj.r.squared
  delta2 <- adj_r2_with_news - adj_r2_no_news
  
  data13 <- rbind(data13,c(list_comp[i], t$statistic, 
                           r2_no_news, r2_with_news,
                           delta1,
                           adj_r2_no_news,adj_r2_with_news,
                           delta2))
}

data13 <- as.data.frame(data13)
data13$t <- as.numeric(data13$t)

#только то что лучше с новостями
df15 <- data13 %>% filter(data13$t < -1.64)
View(df15)
#146 компаний

data11 <- as.data.frame(data11)
df2 <- filter(data11, data11$ticker %in% df15$V1)
#View(df2)

write_xlsx(df2,'значимые_топики_нют_7_дней.xlsx')
write_xlsx(data13,'нют_10_10rep_5_lags.xlsx')


#### ??? ####
library(readxl)
int_10_10rep_7_lags <- read_excel("int_10_10rep_7_lags.xlsx")
View(int_10_10rep_7_lags)
library(readxl)
interfax_df_df <- read_excel("~/Desktop/24_апреля/интерфакс_10_10rep_5_lags.xlsx")
View(интерфакс_10_10rep_5_lags)

data13$V8 <- as.numeric(data13$V8)
interfax_df_df$V8 <- as.numeric(interfax_df_df$V8)

t.test(data13$V8, interfax_df_df$V8)
#нет 


########## NYT и Interfax вместе ################
#### Загрузим список компаний ####
list_comp <- c("ABIO",
               "ABRD",
               "ACKO",
               "AFKS",
               "AFLT",
               "AGRODR",
               "AKRN",
               "ALRS",
               "AMEZ",
               "APTK",
               "AQUA",
               "ARSA",
               "ASSB",
               "AVAN",
               "BANE",
               "BELU",
               "BLNG",
               "BRZL",
               "BSPB",
               "CBOM",
               "CHGZ",
               "CHKZ",
               "CHMF",
               "CHMK",
               "CIANDR",
               "CNTL",
               "DASB",
               "DIOD",
               "DSKY",
               "DVEC",
               "DZRD",
               "EELT",
               "ELFV",
               "ELTZ",
               "ENPG",
               "ETLNDR",
               "FEES",
               "FESH",
               "FIVEDR",
               "FIXPDR",
               "FLOT",
               "FORTUM",
               "GAZA",
               "GAZP",
               "GCHE",
               "GECO",
               "GEMA",
               "GEMCDR",
               "GLTRDR",
               "GMKN",
               "GTRK",
               "HHRUDR",
               "HMSGDR",
               "HYDR",
               "IGST",
               "INGR",
               "IRAO",
               "IRGZ",
               "IRKT",
               "JNOS",
               "KAZT",
               "KBSB",
               "KCHE",
               "KGKC",
               "KLSB",
               "KMAZ",
               "KMEZ",
               "KOGK",
               "KRKN",
               "KROT",
               "KRSB",
               "KTSB",
               "KUBE",
               "KUZB",
               "KZOS",
               "LENT",
               "LIFE",
               "LKOH",
               "LNZL",
               "LPSB",
               "LSNG",
               "LSRG",
               "LVHK",
               "MAGE",
               "MAGN",
               "MDMGDR",
               "MERF",
               "MFGS",
               "MGNT",
               "MGNZ",
               "MGTS",
               "MISB",
               "MOEX",
               "MRKC",
               "MRKK",
               "MRKP",
               "MRKS",
               "MRKU",
               "MRKV",
               "MRKY",
               "MRKZ",
               "MRSB",
               "MSNG",
               "MSRS",
               "MSTT",
               "MTLR",
               "MTSS",
               "MVID",
               "NAUK",
               "NFAZ",
               "NKHP",
               "NKNC",
               "NKSH",
               "NLMK",
               "NMTP",
               "NNSB",
               "NSVZ",
               "NVTK",
               "ODVA",
               "OGKB",
               "OKEYDR",
               "OZONDR",
               "PAZA",
               "PHOR",
               "PIKK",
               "PLZL",
               "PMSB",
               "POLY",
               "POSI",
               "PRFN",
               "PRMB",
               "QIWIDR",
               "RASP",
               "RBCM",
               "RDRB",
               "RENI",
               "RGSS",
               "RKKE",
               "RNFT",
               "ROLO",
               "ROSB",
               "ROSN",
               "ROST",
               "RTGZ",
               "RTKM",
               "RTSB",
               "RUAL",
               "RUGR",
               "RUSI",
               "RZSB",
               "SAGO",
               "SARE",
               "SBER",
               "SELG",
               "SFIN",
               "SGZH",
               "SIBN",
               "SLEN",
               "SMLT",
               "SNGS",
               "SPBE",
               "STSB",
               "SVET",
               "TASB",
               "TATN",
               "TCSGDR",
               "TGKA",
               "TGKB",
               "TGKD",
               "TGKN",
               "TNSE",
               "TORS",
               "TRMK",
               "TTLK",
               "TUZA",
               "UKUZ",
               "UNAC",
               "UNKL",
               "UPRO",
               "URKZ",
               "USBN",
               "UTAR",
               "UWGN",
               "VEON_RX",
               "VGSB",
               "VJGZ",
               "VKCO",
               "VLHZ",
               "VRSB",
               "VSMO",
               "VSYD",
               "VTBR",
               "WTCM",
               "WUSH",
               "YAKG",
               "YKEN",
               "YNDX",
               "YRSB",
               "ZILL",
               "ZVEZ")



length(list_comp)

##### Загрузим доходность с топиками ####
library(reshape)   
library(readr)
library(readxl)
#обратимся к папке и достанем оттуда файлы 
setwd("/Users/macbook/Desktop/Диплом/Доходность с топиками телеграм")
file.list <- list.files(pattern='*.xlsx')
df.list <- lapply(file.list, as.data.frame(read_excel))
View(df.list)

##### линейные модели #####
df450 <- cbind(df10[,1:31],interfax_df_int[-731,2:31])
View(df450)


colnames(df450)[32:61]<- c('interfax_topic_1','interfax_topic_2','interfax_topic_3','interfax_topic_4','interfax_topic_5', 'interfax_topic_6', 
                         'interfax_topic_7','interfax_topic_8', 'interfax_topic_9','interfax_topic_10','interfax_topic_11', 'interfax_topic_12', 
                         'interfax_topic_13','interfax_topic_14','interfax_topic_15', 'interfax_topic_16', 'interfax_topic_17', 'interfax_topic_18',
                         'interfax_topic_19', 'interfax_topic_20','interfax_topic_21','interfax_topic_22','interfax_topic_23','interfax_topic_24', 
                         'interfax_topic_25','interfax_topic_26', 'interfax_topic_27', 'interfax_topic_28','interfax_topic_29','interfax_topic_30')

colnames(df450)[2:31]<- c('nyt_topic_1','nyt_topic_2','nyt_topic_3','nyt_topic_4','nyt_topic_5', 'nyt_topic_6', 
                           'nyt_topic_7','nyt_topic_8', 'nyt_topic_9','nyt_topic_10','nyt_topic_11', 'nyt_topic_12', 
                           'nyt_topic_13','nyt_topic_14','nyt_topic_15', 'nyt_topic_16', 'nyt_topic_17', 'nyt_topic_18',
                           'nyt_topic_19', 'nyt_topic_20','nyt_topic_21','nyt_topic_22','nyt_topic_23','nyt_topic_24', 
                           'nyt_topic_25','nyt_topic_26', 'nyt_topic_27', 'nyt_topic_28','nyt_topic_29','nyt_topic_30')

f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data11 <- NULL
data13 <- NULL

#цикл для AR процесса
for(i in 1:200) {
  cat("Итерация номер", i, "\n")
  
  #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,c(1:11)]
  #изменяем все имена
  names(data_comp) <- gsub("value.", "", names(data_comp))
  
  #отфильтруем даты
  data_comp <- data_comp[order(as.Date(data_comp$data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$data))
  data_comp$data <- as.Date(data_comp$data)
  names(df450)[1] <- 'data'
  
  #объединим словари и доходности
  data_comp <- left_join(df450,data_comp, by = 'data')

  #расставим 0 и 1
  data_comp$ind <- -(is.na(data_comp$price_close)*1 - 1)
  
  #найдем двойные пропуски
  data_comp$ind2 <- 0
  for(j in 2:(dim(data_comp)[1]-1)) {
    data_comp$ind2[j] <- (data_comp$ind[j] == 0 & data_comp$ind[j-1]*data_comp$ind[j+1] == 0)*1
  }
  
  #заполним средним по топикам nyt 
  for(l in 2:31){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(l_for_names[1,1]-1)
    #слово топик для названия
    b <- 'nyt_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  #заполним средним по топикам interfax 
  for(l in 32:61){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(l_for_names[1,1]-31)
    #слово топик для названия
    b <- 'interfax_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  
  #data_comp$d_mean <- data_comp$d
  names(data_comp)
  
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 74:103) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 104:133) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  data_comp <- data_comp[,-c(2:61, 72,73)]
  
  #сделаем лаги топиков
  
  for(s in 12:41) {
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    s_for_names <- as.data.frame(s)
    a <- as.character(s_for_names[1,1]-11)
    #слово топик для названия
    b <- 'nyt_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],1)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
    
    #сделаем второй лаг и название 
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],2)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
    
    #и так далее
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],3)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],4)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],5)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],6)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],7)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
  }
  
  for(s in 42:71) {
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    s_for_names <- as.data.frame(s)
    a <- as.character(s_for_names[1,1]-41)
    #слово топик для названия
    b <- 'interfax_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],1)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
    
    #сделаем второй лаг и название 
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],2)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
    
    #и так далее
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],3)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],4)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],5)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],6)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],7)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
  }
  
  names(data_comp)
  data_comp <- na.omit(data_comp)
  
  data_comp[-1,(dim(data_comp)[2]+1)] <- diff(as.numeric(data_comp[,3]))/na.omit(dplyr::lag(as.numeric(data_comp$price_close), 1))
  #лаги доходности
  data_comp$y_lag_1 <- lag(data_comp[,492], 1)
  data_comp$y_lag_2 <- lag(data_comp[,492], 2)
  data_comp$y_lag_3 <- lag(data_comp[,492], 3)
  data_comp$y_lag_4 <- lag(data_comp[,492], 4)
  data_comp$y_lag_5 <- lag(data_comp[,492], 5)
  data_comp$y_lag_6 <- lag(data_comp[,492], 6)
  data_comp$y_lag_7 <- lag(data_comp[,492], 7)
  
  #удалили лишнее и сделали лаги
  #data2 <- select(data, -ind, -ind2, - d)
  #data2
  
  #удалим месяц когда не работала биржа и еще 7 дней
  
  #установка диапазона дат
  start_date <- as.Date("2022-02-25")
  end_date <- as.Date("2022-04-03")
  
  # Удаление строк с датами в этом диапазоне
  data_comp <- data_comp[!(data_comp$data >= start_date & data_comp$data <= end_date),]
  data_comp <- na.omit(data_comp)
  names(data_comp)
  data1 <- data_comp[,c(12:491)]
  
  #### удалим аномальные доходности####
  N <- which(abs(data_comp[,492])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  f <- c(f,length(N))
  
  #лассо
  mod <- glmnet(data1,data_comp[,492], 
                alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path <- as.data.frame(glmnetPath(mod)$leave)
  a <- max(glmnetPath(mod)$leave$ord)-2
  b <- max(glmnetPath(mod)$leave$ord)
  vec <- as.data.frame(a:b)
  names(vec)[1] <- 'ord'
  
  #отберем значимые топики
  path <- left_join(vec,path,by='ord')
  top <- c(path$var)
  
  #отберем топики
  reg <- data1 %>% dplyr::select(top)
  
  data_y_lags <- as.data.frame(data_comp[,c(492:499)])
  names(data_y_lags)[1] <- 'yield'

  #по лагам
  set.seed(789)
  mod1 <- train(yield~.,
                data=data_y_lags, method = "lm",
                trControl = trainControl(method = 'repeatedcv',
                                         number = 10, repeats = 10))
  
  #достанем значимые лаги
  p_val <- as.data.frame(summary(mod1)$coefficients[-1,4]) 
  names(p_val)[1] <- 'pvalue'
  p_val <- p_val %>% filter(pvalue<0.1)
  lags <- c(rownames(p_val))
  lags <- data_comp %>% dplyr::select(lags)
  dim(lags)
  dim(data_comp)
  dim(reg)
  #сделаем датасет со значимым лагом доходности и значимыми топиками
  all <- as.data.frame(cbind(data_comp[,492], lags,reg))
  names(all)[1] <- 'yield'
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  mod2 <- caret::train(yield ~ .,data=all, 
                       method = "lm",
                       trControl = trainControl(method = 'repeatedcv',
                                                number = 10, repeats = 10))
  summary(mod2)
  
  #достанем значимые лаги
  table_full_AR <- as.data.frame(summary(mod2)$coefficients)
  names(table_full_AR)[4] <- 'P-value'
  g <- grep('topic_', rownames(table_full_AR))
  table <- as.data.frame(table_full_AR[g,])
  table_full_AR <- table %>% dplyr::filter(table[,4] < 0.1)
  table_full_AR_2 <- as.data.frame(rownames(table_full_AR))
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR else cbind(table_full_AR_2,table_full_AR)
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR_1 else table_full_AR_1 %>% mutate(ticker=list_comp[i])
  data11 <- rbind(data11,table_full_AR_1)
  t <- t.test(mod1$resample$Rsquared,mod2$resample$Rsquared)
  
  r2_no_news <- mean(mod1$resample$Rsquared)
  r2_with_news <- mean(mod2$resample$Rsquared)
  delta1 <- r2_with_news - r2_no_news
  
  adj_r2_no_news <- summary(mod1)$adj.r.squared
  adj_r2_with_news <- summary(mod2)$adj.r.squared
  delta2 <- adj_r2_with_news - adj_r2_no_news
  
  data13 <- rbind(data13,c(list_comp[i], t$statistic, 
                           r2_no_news, r2_with_news,
                           delta1,
                           adj_r2_no_news,adj_r2_with_news,
                           delta2))
}



data13 <- as.data.frame(data13)
names(data13)
data13$t <- as.numeric(data13$t)
df15 <- data13 %>% filter(data13$t < -1.64)
View(df15)
#161 компания
data11 <- as.data.frame(data11)
df2 <- filter(data11, data11$ticker %in% df15$V1)
View(df2)

write_xlsx(df2,'значимые_топики_вместе_7_дней.xlsx')
write_xlsx(data13,'вместе_10_10rep_7_lags.xlsx')

prop.test(x=c(150,161), n = c(200,200))

library(readxl)
int1111 <- read_excel("~/Desktop/24_апреля/интерфакс_10_10rep_7_lags.xlsx")
nyt111 <- read_excel("~/Desktop/24_апреля/7 лагов/нют_10_10rep_7_lags.xlsx")

t.test(int1111$V8,nyt111$V8)
t.test(as.numeric(data4444$V8),int1111$V8)
t.test(as.numeric(data4444$V8),nyt111$V8)

data4444 <- data13


prop.test(x=c(150,161), n = c(200,200))
prop.test(x=c(146,161), n = c(200,200))


##### случайный лес #####

df450 <- cbind(df10[,1:31],interfax_df_int[-731,2:31])
View(df450)


colnames(df450)[32:61]<- c('interfax_topic_1','interfax_topic_2','interfax_topic_3','interfax_topic_4','interfax_topic_5', 'interfax_topic_6', 
                           'interfax_topic_7','interfax_topic_8', 'interfax_topic_9','interfax_topic_10','interfax_topic_11', 'interfax_topic_12', 
                           'interfax_topic_13','interfax_topic_14','interfax_topic_15', 'interfax_topic_16', 'interfax_topic_17', 'interfax_topic_18',
                           'interfax_topic_19', 'interfax_topic_20','interfax_topic_21','interfax_topic_22','interfax_topic_23','interfax_topic_24', 
                           'interfax_topic_25','interfax_topic_26', 'interfax_topic_27', 'interfax_topic_28','interfax_topic_29','interfax_topic_30')

colnames(df450)[2:31]<- c('nyt_topic_1','nyt_topic_2','nyt_topic_3','nyt_topic_4','nyt_topic_5', 'nyt_topic_6', 
                          'nyt_topic_7','nyt_topic_8', 'nyt_topic_9','nyt_topic_10','nyt_topic_11', 'nyt_topic_12', 
                          'nyt_topic_13','nyt_topic_14','nyt_topic_15', 'nyt_topic_16', 'nyt_topic_17', 'nyt_topic_18',
                          'nyt_topic_19', 'nyt_topic_20','nyt_topic_21','nyt_topic_22','nyt_topic_23','nyt_topic_24', 
                          'nyt_topic_25','nyt_topic_26', 'nyt_topic_27', 'nyt_topic_28','nyt_topic_29','nyt_topic_30')

f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data11 <- NULL
data13 <- NULL

#цикл для AR процесса
for(i in 1:200) {
  cat("Итерация номер", i, "\n")
  
  #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,c(1:11)]
  #изменяем все имена
  names(data_comp) <- gsub("value.", "", names(data_comp))
  
  #отфильтруем даты
  data_comp <- data_comp[order(as.Date(data_comp$data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$data))
  data_comp$data <- as.Date(data_comp$data)
  names(df450)[1] <- 'data'
  
  #объединим словари и доходности
  data_comp <- left_join(df450,data_comp, by = 'data')
  
  #расставим 0 и 1
  data_comp$ind <- -(is.na(data_comp$price_close)*1 - 1)
  
  #найдем двойные пропуски
  data_comp$ind2 <- 0
  for(j in 2:(dim(data_comp)[1]-1)) {
    data_comp$ind2[j] <- (data_comp$ind[j] == 0 & data_comp$ind[j-1]*data_comp$ind[j+1] == 0)*1
  }
  
  #заполним средним по топикам nyt 
  for(l in 2:31){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(l_for_names[1,1]-1)
    #слово топик для названия
    b <- 'nyt_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  #заполним средним по топикам interfax 
  for(l in 32:61){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(l_for_names[1,1]-31)
    #слово топик для названия
    b <- 'interfax_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  
  #data_comp$d_mean <- data_comp$d
  names(data_comp)
  
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 74:103) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 104:133) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  data_comp <- data_comp[,-c(2:61, 72,73)]
  
  #сделаем лаги топиков
  
  for(s in 12:41) {
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    s_for_names <- as.data.frame(s)
    a <- as.character(s_for_names[1,1]-11)
    #слово топик для названия
    b <- 'nyt_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],1)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
    
    #сделаем второй лаг и название 
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],2)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
    
    #и так далее
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],3)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],4)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],5)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],6)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],7)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
  }
  
  for(s in 42:71) {
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    s_for_names <- as.data.frame(s)
    a <- as.character(s_for_names[1,1]-41)
    #слово топик для названия
    b <- 'interfax_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],1)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
    
    #сделаем второй лаг и название 
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],2)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
    
    #и так далее
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],3)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],4)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],5)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],6)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],7)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
  }
  
  names(data_comp)
  data_comp <- na.omit(data_comp)
  
  data_comp[-1,(dim(data_comp)[2]+1)] <- diff(as.numeric(data_comp[,3]))/na.omit(dplyr::lag(as.numeric(data_comp$price_close), 1))
  #лаги доходности
  data_comp$y_lag_1 <- lag(data_comp[,492], 1)
  data_comp$y_lag_2 <- lag(data_comp[,492], 2)
  data_comp$y_lag_3 <- lag(data_comp[,492], 3)
  data_comp$y_lag_4 <- lag(data_comp[,492], 4)
  data_comp$y_lag_5 <- lag(data_comp[,492], 5)
  data_comp$y_lag_6 <- lag(data_comp[,492], 6)
  data_comp$y_lag_7 <- lag(data_comp[,492], 7)
  
  #удалили лишнее и сделали лаги
  #data2 <- select(data, -ind, -ind2, - d)
  #data2
  
  #удалим месяц когда не работала биржа и еще 7 дней
  
  #установка диапазона дат
  start_date <- as.Date("2022-02-25")
  end_date <- as.Date("2022-04-03")
  
  # Удаление строк с датами в этом диапазоне
  data_comp <- data_comp[!(data_comp$data >= start_date & data_comp$data <= end_date),]
  data_comp <- na.omit(data_comp)
  names(data_comp)
  data1 <- data_comp[,c(12:491)]
  
  #### удалим аномальные доходности####
  N <- which(abs(data_comp[,492])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  f <- c(f,length(N))
  
  #лассо
  mod <- glmnet(data1,data_comp[,492], 
                alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path <- as.data.frame(glmnetPath(mod)$leave)
  a <- max(glmnetPath(mod)$leave$ord)-2
  b <- max(glmnetPath(mod)$leave$ord)
  vec <- as.data.frame(a:b)
  names(vec)[1] <- 'ord'
  
  #отберем значимые топики
  path <- left_join(vec,path,by='ord')
  top <- c(path$var)
  
  #отберем топики
  reg <- data1 %>% dplyr::select(top)
  
  data_y_lags <- as.data.frame(data_comp[,c(492:499)])
  names(data_y_lags)[1] <- 'yield'
  
  #по лагам
  set.seed(789)
  mod1 <- train(yield~.,
                data=data_y_lags, method = "lm",
                trControl = trainControl(method = 'repeatedcv',
                                         number = 10, repeats = 10))
  
  
  #сделаем датасет со значимым лагом доходности и значимыми топиками
  all <- as.data.frame(cbind(data_comp[,492], lags,reg))
  names(all)[1] <- 'yield'
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  mod2 <- caret::train(yield ~ .,data=all, 
                       method = "lm",
                       trControl = trainControl(method = 'repeatedcv',
                                                number = 10, repeats = 10))
  summary(mod2)
  
  #достанем значимые лаги
  table_full_AR <- as.data.frame(summary(mod2)$coefficients)
  names(table_full_AR)[4] <- 'P-value'
  g <- grep('topic_', rownames(table_full_AR))
  table <- as.data.frame(table_full_AR[g,])
  table_full_AR <- table %>% dplyr::filter(table[,4] < 0.1)
  table_full_AR_2 <- as.data.frame(rownames(table_full_AR))
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR else cbind(table_full_AR_2,table_full_AR)
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR_1 else table_full_AR_1 %>% mutate(ticker=list_comp[i])
  data11 <- rbind(data11,table_full_AR_1)
  t <- t.test(mod1$resample$Rsquared,mod2$resample$Rsquared)
  
  r2_no_news <- mean(mod1$resample$Rsquared)
  r2_with_news <- mean(mod2$resample$Rsquared)
  delta1 <- r2_with_news - r2_no_news
  
  adj_r2_no_news <- summary(mod1)$adj.r.squared
  adj_r2_with_news <- summary(mod2)$adj.r.squared
  delta2 <- adj_r2_with_news - adj_r2_no_news
  
  data13 <- rbind(data13,c(list_comp[i], t$statistic, 
                           r2_no_news, r2_with_news,
                           delta1,
                           adj_r2_no_news,adj_r2_with_news,
                           delta2))
}



data13 <- as.data.frame(data13)
names(data13)
data13$t <- as.numeric(data13$t)
df15 <- data13 %>% filter(data13$t < -1.64)
View(df15)

##################### коррелированные словари #####################
df451 <- cbind(df10[,c(1,20,30,24,28,6,11,29)],
               interfax_df_int[-731,c(28,14,19,16,20,23,25)])

write_xlsx(df10,'словари_из_nyt.xlsx')
write_xlsx(interfax_df_int,'словари_из_интерфакса.xlsx')
View(df451)

colnames(df451)[2:8]<- c('interfax_topic_1','interfax_topic_2', 'interfax_topic_3', 
                           'interfax_topic_4', 'interfax_topic_5', 
                           'interfax_topic_6', 'interfax_topic_7')

colnames(df451)[9:15]<- c('nyt_topic_1', 'nyt_topic_2',
                          'nyt_topic_3', 'nyt_topic_4','nyt_topic_5', 
                          'nyt_topic_6', 'nyt_topic_7')

f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data11 <- NULL
data13 <- NULL
data22 <- NULL
#цикл для AR процесса
for(i in 1:200) {
  cat("Итерация номер", i, "\n")
  #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,c(1:11)]
  #изменяем все имена
  names(data_comp) <- gsub("value.", "", names(data_comp))
  
  #отфильтруем даты
  data_comp <- data_comp[order(as.Date(data_comp$data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$data))
  data_comp$data <- as.Date(data_comp$data)
  names(df451)[1] <- 'data'
  
  #объединим словари и доходности
  data_comp <- left_join(df451,data_comp, by = 'data')
  
  #расставим 0 и 1
  data_comp$ind <- -(is.na(data_comp$price_close)*1 - 1)
  
  #найдем двойные пропуски
  data_comp$ind2 <- 0
  for(j in 2:(dim(data_comp)[1]-1)) {
    data_comp$ind2[j] <- (data_comp$ind[j] == 0 & data_comp$ind[j-1]*data_comp$ind[j+1] == 0)*1
  }
  
  names(data_comp)
  #заполним средним по топикам nyt 
  for(l in 2:8){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(colnames(data_comp)[l])
    #слово топик для названия
    b <- 'mean'

    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  #заполним средним по топикам interfax 
  for(l in 9:15){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(colnames(data_comp)[l])
    #слово топик для названия
    b <- 'mean'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  
  #data_comp$d_mean <- data_comp$d
  names(data_comp)
  
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 28:34) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 35:41) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  data_comp <- data_comp[,-c(2:15, 26,27)]
  names(data_comp)

  
  #удалим месяц когда не работала биржа и еще 7 дней
  
  #установка диапазона дат
  start_date <- as.Date("2022-02-25")
  end_date <- as.Date("2022-04-03")
  
  # Удаление строк с датами в этом диапазоне
  data_comp <- data_comp[!(data_comp$data >= start_date & data_comp$data <= end_date),]
  data_comp <- na.omit(data_comp)
  names(data_comp)
  data_comp[-1,(dim(data_comp)[2]+1)] <- diff(as.numeric(data_comp[,3]))/na.omit(dplyr::lag(as.numeric(data_comp$price_close), 1))
  
  #### удалим аномальные доходности####
  N <- which(abs(data_comp[,26])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  f <- c(f,length(N))
  
  
  #сделаем датасет со значимым лагом доходности и значимыми топиками
  all <- as.data.frame(data_comp[,c(26, 12:25)])
  names(all)[1] <- 'yield'
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  
  mod <- randomForest(yield ~., data = all[-1,],
                      trControl = trainControl(method = 'repeatedcv',
                                             number = 10, repeats = 10))
 
  #выведем важность
  a <- importance(mod) %>% data.frame() 
  #создадим дубликат для фильтрации
  k <- a
  #сделаем столбец с названием топиков
  a$topics <- rownames(a)
  #отсортируем по убыванию
  data <- as.data.frame(lapply(k, function(x) x[order(x, decreasing = TRUE)]))
  names(data)[1] <- 'IncNodePurity'
  #получим итоговую таблицу
  a <- left_join(data,a, by='IncNodePurity')
  #создадим таблицу с фильтрацией
  a$n <- 14:1
  #пустой вектор,который будет заполнять
  a$win <- NA
  #если НЙТ побеждает, пиши 1
  if(a$n[a$topics=='mean_nyt_topic_1'] > a$n[a$topics=='mean_interfax_topic_1']) a$win[1]=1 else a$win[1]=0
  if(a$n[a$topics=='mean_nyt_topic_2'] > a$n[a$topics=='mean_interfax_topic_2']) a$win[2]=1 else a$win[2]=0
  if(a$n[a$topics=='mean_nyt_topic_3'] > a$n[a$topics=='mean_interfax_topic_3']) a$win[3]=1 else a$win[3]=0
  if(a$n[a$topics=='mean_nyt_topic_4'] > a$n[a$topics=='mean_interfax_topic_4']) a$win[4]=1 else a$win[4]=0
  if(a$n[a$topics=='mean_nyt_topic_5'] > a$n[a$topics=='mean_interfax_topic_5']) a$win[5]=1 else a$win[5]=0
  if(a$n[a$topics=='mean_nyt_topic_6'] > a$n[a$topics=='mean_interfax_topic_6']) a$win[6]=1 else a$win[6]=0
  if(a$n[a$topics=='mean_nyt_topic_7'] > a$n[a$topics=='mean_interfax_topic_7']) a$win[7]=1 else a$win[7]=0
  
  data22 <- rbind(data22, c(list_comp[i], a$win[1:7]))

  #names(data22)[2] <- 'Места_nyt_из_леса'
  #names(data22)[3] <- 'Места_interfax_из_леса'
}

View(data22)

data22 <- as.data.frame(data22)
data22[,2:8] <- data22[,2:8] %>% mutate_all(as.numeric)
sum(data22$V2)/200*100
sum(data22$V3)/200*100
sum(data22$V4)/200*100
sum(data22$V5)/200*100
sum(data22$V6)/200*100
sum(data22$V7)/200*100
sum(data22$V8)/200*100


интерфакс
df_int1
#нют
df10

#обстрел украины
cor.test(df_int1$topic_3,df10$topic_9)

#беженцы из украины
cor.test(df_int1$topic_5,df10$topic_14)

#ядерное оружие
cor.test(df_int1$topic_10,df10$topic_4)

#зерновая сделка
cor.test(df_int1$topic_15,df10$topic_27)
#значимо + 

#уголовные дела
cor.test(df_int1$topic_19,df10$topic_5)
#значимо отрицательно 

#переговоры
cor.test(df_int1$topic_25,df10$topic_12)
#почти не значимо (положительно)

#удары по крыму
cor.test(df_int1$topic_27,df10$topic_19)
#значимо + 

#европа
cor.test(df_int1$topic_11,df10$topic_6)

#санкции
cor.test(df_int1$topic_22,df10$topic_10)
#значимо положительно

#заявления москвы
cor.test(df_int1$topic_13,df10$topic_29)
#значимо положительно

#война на новых территориях рф
cor.test(df_int1$topic_24,df10$topic_28)
#значимо положительно

################## по периодам ##############
#### Загрузим список компаний ####
list_comp <- c("ABIO",
               "ABRD",
               "ACKO",
               "AFKS",
               "AFLT",
               "AGRODR",
               "AKRN",
               "ALRS",
               "AMEZ",
               "APTK",
               "AQUA",
               "ARSA",
               "ASSB",
               "AVAN",
               "BANE",
               "BELU",
               "BLNG",
               "BRZL",
               "BSPB",
               "CBOM",
               "CHGZ",
               "CHKZ",
               "CHMF",
               "CHMK",
               "CIANDR",
               "CNTL",
               "DASB",
               "DIOD",
               "DSKY",
               "DVEC",
               "DZRD",
               "EELT",
               "ELFV",
               "ELTZ",
               "ENPG",
               "ETLNDR",
               "FEES",
               "FESH",
               "FIVEDR",
               "FIXPDR",
               "FLOT",
               "FORTUM",
               "GAZA",
               "GAZP",
               "GCHE",
               "GECO",
               "GEMA",
               "GEMCDR",
               "GLTRDR",
               "GMKN",
               "GTRK",
               "HHRUDR",
               "HMSGDR",
               "HYDR",
               "IGST",
               "INGR",
               "IRAO",
               "IRGZ",
               "IRKT",
               "JNOS",
               "KAZT",
               "KBSB",
               "KCHE",
               "KGKC",
               "KLSB",
               "KMAZ",
               "KMEZ",
               "KOGK",
               "KRKN",
               "KROT",
               "KRSB",
               "KTSB",
               "KUBE",
               "KUZB",
               "KZOS",
               "LENT",
               "LIFE",
               "LKOH",
               "LNZL",
               "LPSB",
               "LSNG",
               "LSRG",
               "LVHK",
               "MAGE",
               "MAGN",
               "MDMGDR",
               "MERF",
               "MFGS",
               "MGNT",
               "MGNZ",
               "MGTS",
               "MISB",
               "MOEX",
               "MRKC",
               "MRKK",
               "MRKP",
               "MRKS",
               "MRKU",
               "MRKV",
               "MRKY",
               "MRKZ",
               "MRSB",
               "MSNG",
               "MSRS",
               "MSTT",
               "MTLR",
               "MTSS",
               "MVID",
               "NAUK",
               "NFAZ",
               "NKHP",
               "NKNC",
               "NKSH",
               "NLMK",
               "NMTP",
               "NNSB",
               "NSVZ",
               "NVTK",
               "ODVA",
               "OGKB",
               "OKEYDR",
               "OZONDR",
               "PAZA",
               "PHOR",
               "PIKK",
               "PLZL",
               "PMSB",
               "POLY",
               "POSI",
               "PRFN",
               "PRMB",
               "QIWIDR",
               "RASP",
               "RBCM",
               "RDRB",
               "RENI",
               "RGSS",
               "RKKE",
               "RNFT",
               "ROLO",
               "ROSB",
               "ROSN",
               "ROST",
               "RTGZ",
               "RTKM",
               "RTSB",
               "RUAL",
               "RUGR",
               "RUSI",
               "RZSB",
               "SAGO",
               "SARE",
               "SBER",
               "SELG",
               "SFIN",
               "SGZH",
               "SIBN",
               "SLEN",
               "SMLT",
               "SNGS",
               "SPBE",
               "STSB",
               "SVET",
               "TASB",
               "TATN",
               "TCSGDR",
               "TGKA",
               "TGKB",
               "TGKD",
               "TGKN",
               "TNSE",
               "TORS",
               "TRMK",
               "TTLK",
               "TUZA",
               "UKUZ",
               "UNAC",
               "UNKL",
               "UPRO",
               "URKZ",
               "USBN",
               "UTAR",
               "UWGN",
               "VEON_RX",
               "VGSB",
               "VJGZ",
               "VKCO",
               "VLHZ",
               "VRSB",
               "VSMO",
               "VSYD",
               "VTBR",
               "WTCM",
               "WUSH",
               "YAKG",
               "YKEN",
               "YNDX",
               "YRSB",
               "ZILL",
               "ZVEZ")



length(list_comp)

##### Загрузим доходность с топиками ####
library(reshape)   
library(readr)
library(readxl)
#обратимся к папке и достанем оттуда файлы 
setwd("/Users/macbook/Desktop/Доходность с топиками телеграм")
file.list <- list.files(pattern='*.xlsx')
df.list <- lapply(file.list, as.data.frame(read_excel))
View(df.list)

##### линейные модели #####

df450 <- cbind(df10[,1:31],interfax_df_int[-731,2:31])
View(df450)



colnames(df450)[32:61]<- c('interfax_topic_1','interfax_topic_2','interfax_topic_3','interfax_topic_4','interfax_topic_5', 'interfax_topic_6', 
                           'interfax_topic_7','interfax_topic_8', 'interfax_topic_9','interfax_topic_10','interfax_topic_11', 'interfax_topic_12', 
                           'interfax_topic_13','interfax_topic_14','interfax_topic_15', 'interfax_topic_16', 'interfax_topic_17', 'interfax_topic_18',
                           'interfax_topic_19', 'interfax_topic_20','interfax_topic_21','interfax_topic_22','interfax_topic_23','interfax_topic_24', 
                           'interfax_topic_25','interfax_topic_26', 'interfax_topic_27', 'interfax_topic_28','interfax_topic_29','interfax_topic_30')

colnames(df450)[2:31]<- c('nyt_topic_1','nyt_topic_2','nyt_topic_3','nyt_topic_4','nyt_topic_5', 'nyt_topic_6', 
                          'nyt_topic_7','nyt_topic_8', 'nyt_topic_9','nyt_topic_10','nyt_topic_11', 'nyt_topic_12', 
                          'nyt_topic_13','nyt_topic_14','nyt_topic_15', 'nyt_topic_16', 'nyt_topic_17', 'nyt_topic_18',
                          'nyt_topic_19', 'nyt_topic_20','nyt_topic_21','nyt_topic_22','nyt_topic_23','nyt_topic_24', 
                          'nyt_topic_25','nyt_topic_26', 'nyt_topic_27', 'nyt_topic_28','nyt_topic_29','nyt_topic_30')

f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data11 <- NULL
data13 <- NULL

#цикл для AR процесса
for(i in 1:200) {
  cat("Итерация номер", i, "\n")
  
  #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,c(1:11)]
  #изменяем все имена
  names(data_comp) <- gsub("value.", "", names(data_comp))
  
  #отфильтруем даты
  data_comp <- data_comp[order(as.Date(data_comp$data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$data))
  data_comp$data <- as.Date(data_comp$data)
  names(df450)[1] <- 'data'
  
  #объединим словари и доходности
  data_comp <- left_join(df450,data_comp, by = 'data')
  
  #расставим 0 и 1
  data_comp$ind <- -(is.na(data_comp$price_close)*1 - 1)
  
  #найдем двойные пропуски
  data_comp$ind2 <- 0
  for(j in 2:(dim(data_comp)[1]-1)) {
    data_comp$ind2[j] <- (data_comp$ind[j] == 0 & data_comp$ind[j-1]*data_comp$ind[j+1] == 0)*1
  }
  
  #заполним средним по топикам nyt 
  for(l in 2:31){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(l_for_names[1,1]-1)
    #слово топик для названия
    b <- 'nyt_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  #заполним средним по топикам interfax 
  for(l in 32:61){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(l_for_names[1,1]-31)
    #слово топик для названия
    b <- 'interfax_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  
  #data_comp$d_mean <- data_comp$d
  names(data_comp)
  
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 74:103) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 104:133) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  data_comp <- data_comp[,-c(2:61, 72,73)]
  
  #сделаем лаги топиков
  
  for(s in 12:41) {
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    s_for_names <- as.data.frame(s)
    a <- as.character(s_for_names[1,1]-11)
    #слово топик для названия
    b <- 'nyt_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],1)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
    
    #сделаем второй лаг и название 
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],2)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
    
    #и так далее
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],3)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],4)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],5)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],6)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],7)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
  }
  
  for(s in 42:71) {
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    s_for_names <- as.data.frame(s)
    a <- as.character(s_for_names[1,1]-41)
    #слово топик для названия
    b <- 'interfax_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],1)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
    
    #сделаем второй лаг и название 
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],2)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
    
    #и так далее
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],3)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],4)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],5)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],6)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],7)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
  }
  
  names(data_comp)
  data_comp <- na.omit(data_comp)
  
  data_comp[-1,(dim(data_comp)[2]+1)] <- diff(as.numeric(data_comp[,3]))/na.omit(dplyr::lag(as.numeric(data_comp$price_close), 1))
  #лаги доходности
  data_comp$y_lag_1 <- lag(data_comp[,492], 1)
  data_comp$y_lag_2 <- lag(data_comp[,492], 2)
  data_comp$y_lag_3 <- lag(data_comp[,492], 3)
  data_comp$y_lag_4 <- lag(data_comp[,492], 4)
  data_comp$y_lag_5 <- lag(data_comp[,492], 5)
  data_comp$y_lag_6 <- lag(data_comp[,492], 6)
  data_comp$y_lag_7 <- lag(data_comp[,492], 7)
  
  #удалили лишнее и сделали лаги
  #data2 <- select(data, -ind, -ind2, - d)
  #data2
  
  #удалим месяц когда не работала биржа и еще 7 дней
  
  #установка диапазона дат
  start_date <- as.Date("2022-02-25")
  end_date <- as.Date("2022-04-03")
  
  # Удаление строк с датами в этом диапазоне
  data_comp <- data_comp[!(data_comp$data >= start_date & data_comp$data <= end_date),]
  data_comp <- na.omit(data_comp)
  names(data_comp)
  data1 <- data_comp[,c(12:491)]
  
  #### удалим аномальные доходности####
  N <- which(abs(data_comp[,492])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  f <- c(f,length(N))
  
  #лассо
  mod <- glmnet(data1,data_comp[,492], 
                alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path <- as.data.frame(glmnetPath(mod)$leave)
  a <- max(glmnetPath(mod)$leave$ord)-2
  b <- max(glmnetPath(mod)$leave$ord)
  vec <- as.data.frame(a:b)
  names(vec)[1] <- 'ord'
  
  #отберем значимые топики
  path <- left_join(vec,path,by='ord')
  top <- c(path$var)
  
  #отберем топики
  reg <- data1 %>% dplyr::select(top)
  
  data_y_lags <- as.data.frame(data_comp[,c(492:499)])
  names(data_y_lags)[1] <- 'yield'
  
  #по лагам
  set.seed(789)
  mod1 <- train(yield~.,
                data=data_y_lags, method = "lm",
                trControl = trainControl(method = 'repeatedcv',
                                         number = 10, repeats = 10))
  
  #достанем значимые лаги
  p_val <- as.data.frame(summary(mod1)$coefficients[-1,4]) 
  names(p_val)[1] <- 'pvalue'
  p_val <- p_val %>% filter(pvalue<0.1)
  lags <- c(rownames(p_val))
  lags <- data_comp %>% dplyr::select(lags)
  dim(lags)
  dim(data_comp)
  dim(reg)
  #сделаем датасет со значимым лагом доходности и значимыми топиками
  all <- as.data.frame(cbind(data_comp[,492], lags,reg))
  names(all)[1] <- 'yield'
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  mod2 <- caret::train(yield ~ .,data=all, 
                       method = "lm",
                       trControl = trainControl(method = 'repeatedcv',
                                                number = 10, repeats = 10))
  summary(mod2)
  
  #достанем значимые лаги
  table_full_AR <- as.data.frame(summary(mod2)$coefficients)
  names(table_full_AR)[4] <- 'P-value'
  g <- grep('topic_', rownames(table_full_AR))
  table <- as.data.frame(table_full_AR[g,])
  table_full_AR <- table %>% dplyr::filter(table[,4] < 0.1)
  table_full_AR_2 <- as.data.frame(rownames(table_full_AR))
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR else cbind(table_full_AR_2,table_full_AR)
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR_1 else table_full_AR_1 %>% mutate(ticker=list_comp[i])
  data11 <- rbind(data11,table_full_AR_1)
  t <- t.test(mod1$resample$Rsquared,mod2$resample$Rsquared)
  
  r2_no_news <- mean(mod1$resample$Rsquared)
  r2_with_news <- mean(mod2$resample$Rsquared)
  delta1 <- r2_with_news - r2_no_news
  
  adj_r2_no_news <- summary(mod1)$adj.r.squared
  adj_r2_with_news <- summary(mod2)$adj.r.squared
  delta2 <- adj_r2_with_news - adj_r2_no_news
  
  data13 <- rbind(data13,c(list_comp[i], t$statistic, 
                           r2_no_news, r2_with_news,
                           delta1,
                           adj_r2_no_news,adj_r2_with_news,
                           delta2))
}



data13 <- as.data.frame(data13)
names(data13)
data13$t <- as.numeric(data13$t)
df15 <- data13 %>% filter(data13$t < -1.64)
View(df15)
#161 компания
data11 <- as.data.frame(data11)
df2 <- filter(data11, data11$ticker %in% df15$V1)
View(df2)

write_xlsx(df2,'значимые_топики_вместе_7_дней.xlsx')
write_xlsx(data13,'вместе_10_10rep_7_lags.xlsx')

prop.test(x=c(150,161), n = c(200,200))


library(readxl)
int1111 <- read_excel("~/Desktop/24_апреля/интерфакс_10_10rep_7_lags.xlsx")
nyt111 <- read_excel("~/Desktop/24_апреля/7 лагов/нют_10_10rep_7_lags.xlsx")

t.test(int1111$V8,nyt111$V8)
t.test(as.numeric(data4444$V8),int1111$V8)
t.test(as.numeric(data4444$V8),nyt111$V8)

data4444 <- data13


prop.test(x=c(150,161), n = c(200,200))
prop.test(x=c(146,161), n = c(200,200))


##### случайный лес #####
library(readxl)
df10 <- read_excel("~/Desktop/словари_из_nyt.xlsx")
interfax_df_int <- read_excel("~/Desktop/словари_из_интерфакса.xlsx")

df450 <- cbind(df10[,1:31],interfax_df_int[-731,2:31])
View(df450)


colnames(df450)[32:61]<- c('interfax_topic_1','interfax_topic_2','interfax_topic_3','interfax_topic_4','interfax_topic_5', 'interfax_topic_6', 
                           'interfax_topic_7','interfax_topic_8', 'interfax_topic_9','interfax_topic_10','interfax_topic_11', 'interfax_topic_12', 
                           'interfax_topic_13','interfax_topic_14','interfax_topic_15', 'interfax_topic_16', 'interfax_topic_17', 'interfax_topic_18',
                           'interfax_topic_19', 'interfax_topic_20','interfax_topic_21','interfax_topic_22','interfax_topic_23','interfax_topic_24', 
                           'interfax_topic_25','interfax_topic_26', 'interfax_topic_27', 'interfax_topic_28','interfax_topic_29','interfax_topic_30')

colnames(df450)[2:31]<- c('nyt_topic_1','nyt_topic_2','nyt_topic_3','nyt_topic_4','nyt_topic_5', 'nyt_topic_6', 
                          'nyt_topic_7','nyt_topic_8', 'nyt_topic_9','nyt_topic_10','nyt_topic_11', 'nyt_topic_12', 
                          'nyt_topic_13','nyt_topic_14','nyt_topic_15', 'nyt_topic_16', 'nyt_topic_17', 'nyt_topic_18',
                          'nyt_topic_19', 'nyt_topic_20','nyt_topic_21','nyt_topic_22','nyt_topic_23','nyt_topic_24', 
                          'nyt_topic_25','nyt_topic_26', 'nyt_topic_27', 'nyt_topic_28','nyt_topic_29','nyt_topic_30')

f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data11 <- NULL
data13 <- NULL
library(lubridate)
library(dplyr)
library(glmnet)
library(solzy)
library(caret)
library(MASS)
library(randomForest)
#цикл для AR процесса
for(i in 1:200) {
  cat("Итерация номер", i, "\n")
  #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,c(1:11)]
  #изменяем все имена
  names(data_comp) <- gsub("value.", "", names(data_comp))
  
  #отфильтруем даты
  data_comp <- data_comp[order(as.Date(data_comp$data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$data))
  data_comp$data <- as.Date(data_comp$data)
  names(df450)[1] <- 'data'
  
  #объединим словари и доходности
  data_comp <- left_join(df450,data_comp, by = 'data')
  
  #расставим 0 и 1
  data_comp$ind <- -(is.na(data_comp$price_close)*1 - 1)
  
  #найдем двойные пропуски
  data_comp$ind2 <- 0
  for(j in 2:(dim(data_comp)[1]-1)) {
    data_comp$ind2[j] <- (data_comp$ind[j] == 0 & data_comp$ind[j-1]*data_comp$ind[j+1] == 0)*1
  }
  
  #заполним средним по топикам nyt 
  for(l in 2:31){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(l_for_names[1,1]-1)
    #слово топик для названия
    b <- 'nyt_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  #заполним средним по топикам interfax 
  for(l in 32:61){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(l_for_names[1,1]-31)
    #слово топик для названия
    b <- 'interfax_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  
  #data_comp$d_mean <- data_comp$d
  names(data_comp)
  
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 74:103) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 104:133) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  data_comp <- data_comp[,-c(2:61, 72,73)]
  
  #сделаем лаги топиков
  
  for(s in 12:41) {
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    s_for_names <- as.data.frame(s)
    a <- as.character(s_for_names[1,1]-11)
    #слово топик для названия
    b <- 'nyt_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],1)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
    
    #сделаем второй лаг и название 
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],2)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
    
    #и так далее
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],3)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],4)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],5)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],6)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],7)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
  }
  
  for(s in 42:71) {
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    s_for_names <- as.data.frame(s)
    a <- as.character(s_for_names[1,1]-41)
    #слово топик для названия
    b <- 'interfax_mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],1)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
    
    #сделаем второй лаг и название 
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],2)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
    
    #и так далее
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],3)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],4)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],5)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],6)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],7)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
  }
  
  names(data_comp)
  data_comp <- na.omit(data_comp)
  
  data_comp[-1,(dim(data_comp)[2]+1)] <- diff(as.numeric(data_comp[,3]))/na.omit(dplyr::lag(as.numeric(data_comp$price_close), 1))
  #лаги доходности
  data_comp$y_lag_1 <- lag(data_comp[,492], 1)
  data_comp$y_lag_2 <- lag(data_comp[,492], 2)
  data_comp$y_lag_3 <- lag(data_comp[,492], 3)
  data_comp$y_lag_4 <- lag(data_comp[,492], 4)
  
  data_comp$y_lag_5 <- lag(data_comp[,492], 5)
  data_comp$y_lag_6 <- lag(data_comp[,492], 6)
  data_comp$y_lag_7 <- lag(data_comp[,492], 7)
  
  #удалили лишнее и сделали лаги
  #data2 <- select(data, -ind, -ind2, - d)
  #data2
  
  #удалим месяц когда не работала биржа и еще 7 дней
  
  #установка диапазона дат
  start_date <- as.Date("2022-02-25")
  end_date <- as.Date("2022-04-03")
  
  # Удаление строк с датами в этом диапазоне
  data_comp <- data_comp[!(data_comp$data >= start_date & data_comp$data <= end_date),]
  data_comp <- na.omit(data_comp)
  names(data_comp)
  data1 <- data_comp[,c(12:491)]
  
  #### удалим аномальные доходности####
  N <- which(abs(data_comp[,492])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  f <- c(f,length(N))

  
  data_y_lags <- as.data.frame(data_comp[,c(492:499)])
  names(data_y_lags)[1] <- 'yield'
  
  #по лагам
  set.seed(789)
  mod1 <- train(yield~.,
                data=data_y_lags, method = "gbm",
                trControl = trainControl(method = 'repeatedcv',
                                         number = 10, repeats = 10))
  
  mod1$resample$Rsquared
  
  #отберем значимые топики
  for_filter <- cbind(data_comp[,492],data1)
  names(for_filter)[1] <- 'yield'
  
  mod <- train(yield~.,
               data=for_filter, method = "gbm",
               trControl = trainControl(method = 'repeatedcv',
                                        number = 10, repeats = 10))
  
  importance <- caret::varImp(mod$finalModel, scale = FALSE)
  
  #выведем важность
  a <- importance %>% data.frame() 
  #создадим дубликат для фильтрации
  k <- a
  k$topics <- rownames(a)
  #сделаем столбец с названием топиков
  a$topics <- rownames(a)
  sorted_data <- k[order(-k$Overall), ]
  top <- k$topics[k$Overall !=0]
  
  
  #отберем топики
  reg <- data1 %>% dplyr::select(top)

  all <- cbind(data_y_lags,reg)
  names(all)[1] <- 'yield'
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  mod2 <- caret::train(yield ~ .,data=all, 
                       method = "gbm",
                       trControl = trainControl(method = 'repeatedcv',
                                                number = 10, repeats = 10))
  mod2$resample$Rsquared
  
  #
  t <- t.test(mod1$resample$Rsquared,mod2$resample$Rsquared)
  
  r2_no_news <- mean(mod1$resample$Rsquared)
  r2_with_news <- mean(mod2$resample$Rsquared)
  delta1 <- r2_with_news - r2_no_news
  
  data13 <- rbind(data13,c(list_comp[i], t$statistic, 
                           r2_no_news, r2_with_news,
                           delta1))
}



data22 <- NULL
data22 <- rbind(data22, c(list_comp[i], top))
View(data22)

data13 <- as.data.frame(data13)
write_xlsx(data13,'вместе_boosting.xlsx')
library(writexl)

names(data13)

data13$t <- as.numeric(data13$t)
df15 <- data13 %>% filter(data13$t < -1.64)
View(df15)

#### на 4 периодах ####
#### Список компаний ДЛЯ 4 ПЕРИОДОВ ####
list_comp <- c("ABIO",
               "ABRD",
               "AFKS",
               "AFLT",
               "AGRODR",
               "AKRN",
               "ALRS",
               "AMEZ",
               "APTK",
               "AQUA",
               "ARSA",
               "ASSB",
               "AVAN",
               "BANE",
               "BELU",
               "BLNG",
               "BRZL",
               "BSPB",
               "CBOM",
               "CHGZ",
               "CHKZ",
               "CHMF",
               "CHMK",
               "CIANDR",
               "CNTL",
               "DIOD",
               "DSKY",
               "DVEC",
               "DZRD",
               "EELT",
               "ELFV",
               "ELTZ",
               "ENPG",
               "ETLNDR",
               "FEES",
               "FESH",
               "FIVEDR",
               "FIXPDR",
               "FLOT",
               "FORTUM",
               "GAZA",
               "GAZP",
               "GCHE",
               "GEMA",
               "GEMCDR",
               "GLTRDR",
               "GMKN",
               "GTRK",
               "HHRUDR",
               "HMSGDR",
               "HYDR",
               "IGST",
               "INGR",
               "IRAO",
               "IRKT",
               "JNOS",
               "KAZT",
               "KBSB",
               "KCHE",
               "KGKC",
               "KLSB",
               "KMAZ",
               "KMEZ",
               "KOGK",
               "KRKN",
               "KROT",
               "KRSB",
               "KTSB",
               "KUBE",
               "KUZB",
               "KZOS",
               "LENT",
               "LIFE",
               "LKOH",
               "LNZL",
               "LPSB",
               "LSNG",
               "LSRG",
               "LVHK",
               "MAGE",
               "MAGN",
               "MDMGDR",
               "MERF",
               "MFGS",
               "MGNT",
               "MGTS",
               "MISB",
               "MOEX",
               "MRKC",
               "MRKK",
               "MRKP",
               "MRKS",
               "MRKU",
               "MRKV",
               "MRKY",
               "MRKZ",
               "MRSB",
               "MSNG",
               "MSRS",
               "MSTT",
               "MTLR",
               "MTSS",
               "MVID",
               "NAUK",
               "NFAZ",
               "NKHP",
               "NKNC",
               "NKSH",
               "NLMK",
               "NMTP",
               "NNSB",
               "NSVZ",
               "NVTK",
               "ODVA",
               "OGKB",
               "OKEYDR",
               "OZONDR",
               "PAZA",
               "PHOR",
               "PIKK",
               "PLZL",
               "PMSB",
               "POLY",
               "POSI",
               "PRFN",
               "PRMB",
               "QIWIDR",
               "RASP",
               "RBCM",
               "RDRB",
               "RENI",
               "RGSS",
               "RKKE",
               "RNFT",
               "ROLO",
               "ROSB",
               "ROSN",
               "ROST",
               "RTGZ",
               "RTKM",
               "RTSB",
               "RUAL",
               "RUGR",
               "RUSI",
               "RZSB",
               "SAGO",
               "SARE",
               "SBER",
               "SELG",
               "SFIN",
               "SGZH",
               "SIBN",
               "SLEN",
               "SMLT",
               "SNGS",
               "SPBE",
               "STSB",
               "SVET",
               "TASB",
               "TATN",
               "TCSGDR",
               "TGKA",
               "TGKB",
               "TGKN",
               "TNSE",
               "TORS",
               "TRMK",
               "TTLK",
               "TUZA",
               "UKUZ",
               "UNAC",
               "UNKL",
               "UPRO",
               "URKZ",
               "USBN",
               "UTAR",
               "UWGN",
               "VEON_RX",
               "VGSB",
               "VJGZ",
               "VKCO",
               "VLHZ",
               "VRSB",
               "VSMO",
               "VSYD",
               "VTBR",
               "WTCM",
               "YAKG",
               "YKEN",
               "YNDX",
               "YRSB",
               "ZILL",
               "ZVEZ")

length(list_comp)
#### загрузим для 4 периодов ####
library(reshape)   
library(readr)
library(readxl)
#обратимся к папке и достанем оттуда файлы 
setwd("/Users/macbook/Desktop/Доходность с топиками телеграм 4 периода")
file.list <- list.files(pattern='*.xlsx')
df.list <- lapply(file.list, as.data.frame(read_excel))
View(df.list)


library(readxl)

df10 <- read_excel("Desktop/словари_из_nyt.xlsx")
View(df10)
#### AR для 4 периодов ####
f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data17 <- NULL
data11 <- NULL
data18 <- NULL
data19 <- NULL
data20 <- NULL
#цикл для AR процесса
for(i in 1:193) {
  cat("Итерация номер", i, "\n")

   #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,c(1:11)]
  #изменяем все имена
  names(data_comp) <- gsub("value.", "", names(data_comp))
  
  #отфильтруем даты
  data_comp <- data_comp[order(as.Date(data_comp$data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$data))
  data_comp$data <- as.Date(data_comp$data)
  names(df10)[1] <- 'data'
  
  #объединим словари и доходности
  data_comp <- left_join(df10,data_comp, by = 'data')
  
  
  #расставим 0 и 1
  data_comp$ind <- -(is.na(data_comp$price_close)*1 - 1)
  
  #найдем двойные пропуски
  data_comp$ind2 <- 0
  for(j in 2:(dim(data_comp)[1]-1)) {
    data_comp$ind2[j] <- (data_comp$ind[j] == 0 & data_comp$ind[j-1]*data_comp$ind[j+1] == 0)*1
  }
  
  #заполним средним
  for(l in 2:31){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(l_for_names[1,1]-1)
    #слово топик для названия
    b <- 'mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  #data_comp$d_mean <- data_comp$d
  names(data_comp)
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 44:73) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  data_comp <- data_comp[,-c(2:31, 42,43)]
  
  #сделаем лаги топиков
  
  for(s in 12:41) {
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    s_for_names <- as.data.frame(s)
    a <- as.character(s_for_names[1,1]-11)
    #слово топик для названия
    b <- 'mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],1)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
    
    #сделаем второй лаг и название 
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],2)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
    
    #и так далее
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],3)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],4)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],5)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],6)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],7)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
  }
  
  names(data_comp)
  data_comp <- na.omit(data_comp)
  
  data_comp[-1,(dim(data_comp)[2]+1)] <- diff(as.numeric(data_comp$price_close))/na.omit(dplyr::lag(as.numeric(data_comp$price_close), 1))
  names(data_comp)[252] <- 'yield'
  data_comp$yield <- as.numeric(data_comp$yield)
  
  #лаги доходности
  data_comp$y_lag_1 <- lag(data_comp$yield, 1)
  data_comp$y_lag_2 <- lag(data_comp$yield, 2)
  data_comp$y_lag_3 <- lag(data_comp$yield, 3)
  data_comp$y_lag_4 <- lag(data_comp$yield, 4)
  data_comp$y_lag_5 <- lag(data_comp$yield, 5)
  data_comp$y_lag_6 <- lag(data_comp$yield, 6)
  data_comp$y_lag_7 <- lag(data_comp$yield, 7)
  
  #установка диапазона дат
  start_date <- as.Date("2022-02-25")
  end_date <- as.Date("2022-04-03")
  
  # Удаление строк с датами в этом диапазоне
  data_comp <- data_comp[!(data_comp$data >= start_date & data_comp$data <= end_date),]
  
  data_comp <- na.omit(data_comp)
  
  data1 <- data_comp[,c(12:251)]
  data_y_lags <- data_comp[,c(252:259)]
  
  #####
  N <- which(abs(data_comp[,252])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  data_y_lags <- if(length(N)>0) data_y_lags[-N,] else data_y_lags
  f <- c(f,length(N))
  names(data_comp)
  
  data_comp <- as.data.frame(data_comp)
  
  data_comp[,12:259] <- data_comp[,12:259] %>% mutate_all(as.numeric)
  
  
  ##### разделим на 4 периода  ####
  dim_of_data <- as.data.frame(t(as.data.frame(dim(data_comp))))
  names(dim_of_data)[1] <- 'number'
  for_filter <- round(as.numeric(dim_of_data$number/4))
  
  data_comp1 <- data_comp[c(1:for_filter),] %>% as.data.frame()
  data_comp2 <- data_comp[c((for_filter+1):(2*for_filter)),]  %>% as.data.frame()
  data_comp3 <- data_comp[c((2*for_filter+1):(3*for_filter)),]  %>% as.data.frame()
  data_comp4 <- data_comp[c((3*for_filter+1):(dim_of_data$number)),] %>% as.data.frame()
  
  data1_1 <- data1[c(1:for_filter),] %>% as.data.frame()
  data1_2 <- data1[c((for_filter+1):(2*for_filter)),] %>% as.data.frame()
  data1_3 <- data1[c((2*for_filter+1):(3*for_filter)),] %>% as.data.frame()
  data1_4 <- data1[c((3*for_filter+1):(dim_of_data$number)),] %>% as.data.frame()
  
  data_y_lags1 <- data_y_lags[c(1:for_filter),] %>% as.data.frame()
  data_y_lags2 <- data_y_lags[c((for_filter+1):(2*for_filter)),] %>% as.data.frame()
  data_y_lags3 <- data_y_lags[c((2*for_filter+1):(3*for_filter)),] %>% as.data.frame()
  data_y_lags4 <- data_y_lags[c((3*for_filter+1):(dim_of_data$number)),] %>% as.data.frame()
  
  
  #### лассо для 1 периода ####
  mod1 <- glmnet(data1_1,data_comp1[,252], 
                 alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path1 <- as.data.frame(glmnetPath(mod1)$leave)
  a1 <- max(glmnetPath(mod1)$leave$ord)-2
  b1 <- max(glmnetPath(mod1)$leave$ord)
  vec1 <- as.data.frame(a1:b1)
  names(vec1)[1] <- 'ord'
  
  #отберем значимые топики
  path1 <- left_join(vec1,path1,by='ord')
  top1 <- c(path1$var)
  
  #отберем топики
  reg1 <- data1_1 %>% dplyr::select(top1)
  
  
  #### лассо для 2 периода ####
  mod2 <- glmnet(data1_2,data_comp2[,252], 
                 alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path2 <- as.data.frame(glmnetPath(mod2)$leave)
  a2 <- max(glmnetPath(mod2)$leave$ord)-2
  b2 <- max(glmnetPath(mod2)$leave$ord)
  vec2 <- as.data.frame(a2:b2)
  names(vec2)[1] <- 'ord'
  
  #отберем значимые топики
  path2 <- left_join(vec2,path2,by='ord')
  top2 <- c(path2$var)
  
  #отберем топики
  reg2 <- data1_2 %>% dplyr::select(top2)
  
  
  #### лассо для 3 периода ####
  mod3 <- glmnet(data1_3,data_comp3[,252], 
                 alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path3 <- as.data.frame(glmnetPath(mod3)$leave)
  a3 <- max(glmnetPath(mod3)$leave$ord)-2
  b3 <- max(glmnetPath(mod3)$leave$ord)
  vec3 <- as.data.frame(a3:b3)
  names(vec3)[1] <- 'ord'
  
  #отберем значимые топики
  path3 <- left_join(vec3,path3,by='ord')
  top3 <- c(path3$var)
  
  #отберем топики
  reg3 <- data1_3 %>% dplyr::select(top3)
  
  
  #### лассо для 4 периода ####
  mod4 <- glmnet(data1_4,data_comp4[,252], 
                 alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path4 <- as.data.frame(glmnetPath(mod4)$leave)
  a4 <- max(glmnetPath(mod4)$leave$ord)-2
  b4 <- max(glmnetPath(mod4)$leave$ord)
  vec4 <- as.data.frame(a4:b4)
  names(vec4)[1] <- 'ord'
  
  #отберем значимые топики
  path4 <- left_join(vec4,path4,by='ord')
  top4 <- c(path4$var)
  
  #отберем топики
  reg4 <- data1_4 %>% dplyr::select(top4)
  
  
  #1 период
  #только по топикам
  set.seed(789)
  mod1_1 <- train(yield~.,
                  data=data_y_lags1, method = "lm",
                  trControl = trainControl(method = 'repeatedcv',
                                           number = 10, repeats = 10))
  
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  all_1 <- cbind(data_y_lags1, reg1)
  mod1_2 <- caret::train(yield ~ .,data=all_1, 
                         method = "lm",
                         trControl = trainControl(method = 'repeatedcv',
                                                  number = 10, repeats = 10))
  
  #2 период
  #только по топикам
  set.seed(789)
  mod2_1 <- train(yield~.,
                  data=data_y_lags2, method = "lm",
                  trControl = trainControl(method = 'repeatedcv',
                                           number = 10, repeats = 10))
  
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  all_2 <- cbind(data_y_lags2, reg2)
  mod2_2 <- caret::train(yield ~ .,data=all_2, 
                         method = "lm",
                         trControl = trainControl(method = 'repeatedcv',
                                                  number = 10, repeats = 10))
  #3 период
  #только по топикам
  set.seed(789)
  mo3_1 <- train(yield~.,
                 data=data_y_lags3, method = "lm",
                 trControl = trainControl(method = 'repeatedcv',
                                          number = 10, repeats = 10))
  
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  all_3 <- cbind(data_y_lags3, reg3)
  mod3_2 <- caret::train(yield ~ .,data=all_3, 
                         method = "lm",
                         trControl = trainControl(method = 'repeatedcv',
                                                  number = 10, repeats =10))
  
  #4 период
  #только по топикам
  set.seed(789)
  mo4_1 <- train(yield~.,
                 data=data_y_lags4, method = "lm",
                 trControl = trainControl(method = 'repeatedcv',
                                          number = 10, repeats = 10))
  
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  all_4 <- cbind(data_y_lags4, reg4)
  mod4_2 <- caret::train(yield ~ .,data=all_4, 
                         method = "lm",
                         trControl = trainControl(method = 'repeatedcv',
                                                  number = 10, repeats = 10))
  
  
  t1 <- t.test(mod1_1$resample$Rsquared,mod1_2$resample$Rsquared)
  r2_no_news1 <-  summary(mod1_1)$adj.r.squared
  r2_with_news1 <- summary(mod1_2)$adj.r.squared
  delta1 <- r2_with_news1 - r2_no_news1
  
  t2 <- t.test(mod2_1$resample$Rsquared,mod2_2$resample$Rsquared)
  r2_no_news2 <-  summary(mod2_1)$adj.r.squared
  r2_with_news2 <- summary(mod2_2)$adj.r.squared
  delta2 <- r2_with_news2 - r2_no_news2
  
  t3 <- t.test(mo3_1$resample$Rsquared,mod3_2$resample$Rsquared)
  r2_no_news3 <-  summary(mo3_1)$adj.r.squared
  r2_with_news3 <- summary(mod3_2)$adj.r.squared
  delta3 <- r2_with_news3 - r2_no_news3
  
  t4 <- t.test(mo4_1$resample$Rsquared,mod4_2$resample$Rsquared)
  r2_no_news4 <-  summary(mo4_1)$adj.r.squared
  r2_with_news4 <- summary(mod4_2)$adj.r.squared
  delta4 <- r2_with_news4 - r2_no_news4
  
  period <- as.data.frame(for_filter)
  names(period)[1] <- 'Длина одного периода'
  
  #достанем значимые лаги 1 период
  table_full_AR_3 <- as.data.frame(summary(mod1_2)$coefficients)
  names(table_full_AR_3)[4] <- 'P-value'
  g3 <- grep('mean_topic', rownames(table_full_AR_3))
  table3 <- as.data.frame(table_full_AR_3[g3,])
  table_full_AR_3 <- table3 %>% dplyr::filter(table3[,4] < 0.1)
  table_full_AR_3_2 <- as.data.frame(rownames(table_full_AR_3))
  table_full_AR_3_1 <- if(dim(table_full_AR_3)[1]==0) table_full_AR_3 else cbind(table_full_AR_3_2,table_full_AR_3)
  table_full_AR_3_1 <- if(dim(table_full_AR_3)[1]==0) table_full_AR_3 else table_full_AR_3_1 %>% mutate(ticker=list_comp[i])
  data17 <- rbind(data17,table_full_AR_3_1)
  
  #достанем значимые лаги 2 период
  table_full_AR <- as.data.frame(summary(mod2_2)$coefficients)
  names(table_full_AR)[4] <- 'P-value'
  g <- grep('mean_topic', rownames(table_full_AR))
  table <- as.data.frame(table_full_AR[g,])
  table_full_AR <- table %>% dplyr::filter(table[,4] < 0.1)
  table_full_AR_2 <- as.data.frame(rownames(table_full_AR))
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR else cbind(table_full_AR_2,table_full_AR)
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR_1 else table_full_AR_1 %>% mutate(ticker=list_comp[i])
  data18 <- rbind(data18,table_full_AR_1)
  
  
  #достанем значимые лаги 3 период
  table_full_AR_3 <- as.data.frame(summary(mod3_2)$coefficients)
  names(table_full_AR_3)[4] <- 'P-value'
  g3 <- grep('mean_topic', rownames(table_full_AR_3))
  table3 <- as.data.frame(table_full_AR_3[g3,])
  table_full_AR_3 <- table3 %>% dplyr::filter(table3[,4] < 0.1)
  table_full_AR_3_2 <- as.data.frame(rownames(table_full_AR_3))
  table_full_AR_3_1 <- if(dim(table_full_AR_3)[1]==0) table_full_AR_3 else cbind(table_full_AR_3_2,table_full_AR_3)
  table_full_AR_3_1 <- if(dim(table_full_AR_3)[1]==0) table_full_AR_3 else table_full_AR_3_1 %>% mutate(ticker=list_comp[i])
  data19 <- rbind(data19,table_full_AR_3_1)
  
  #достанем значимые лаги 4 период
  table_full_AR <- as.data.frame(summary(mod4_2)$coefficients)
  names(table_full_AR)[4] <- 'P-value'
  g <- grep('mean_topic', rownames(table_full_AR))
  table <- as.data.frame(table_full_AR[g,])
  table_full_AR <- table %>% dplyr::filter(table[,4] < 0.1)
  table_full_AR_2 <- as.data.frame(rownames(table_full_AR))
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR else cbind(table_full_AR_2,table_full_AR)
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR_1 else table_full_AR_1 %>% mutate(ticker=list_comp[i])
  data20 <- rbind(data20,table_full_AR_1)
  
  data9 <- rbind(data9,c(list_comp[i], 
                         t1$statistic,r2_no_news1,r2_with_news1,delta1,
                         t2$statistic,r2_no_news2,r2_with_news2,delta2,
                         t3$statistic,r2_no_news3,r2_with_news3,delta3,
                         t4$statistic,r2_no_news4,r2_with_news4,delta4,
                         period))}

data17 <- data17 %>% as.data.frame()
data18 <- data18 %>% as.data.frame()
data19 <- data19 %>% as.data.frame()
data20 <- data20 %>% as.data.frame()

data9 <- as.data.frame(data9)
View(data9)
setwd('/Users/macbook/Desktop/По периодам')

write_xlsx(data17, 'int_1_период_топики_AR.xlsx')
write_xlsx(data18, 'int_2_период_топики_AR.xlsx')
write_xlsx(data19, 'int_3_период_топики_AR.xlsx')
write_xlsx(data20, 'int_4_период_топики_AR.xlsx')
write_xlsx(data9, 'int_свод_AR.xlsx')




#### график ####
library(readxl)
AR_4_periods <- read_excel("/Users/macbook/Desktop/Текущее/апрель_ТГ_журнал_ВШЭ/1_april_4_periods.xlsx")
#View(AR_4_periods)
library(ggplot2)
library(ComplexUpset)
library(gt)


columns <- colnames(AR_4_periods)[-1]
names(AR_4_periods)
grap <- upset(AR_4_periods, columns, 
      name = 'Пересечения периодов',
      base_annotations = list("Количество компаний" = intersection_size(fill = "#153f65")), 
      set_sizes=upset_set_size() + ylab('Влияние новостей в каждом периоде'),
      queries = list(upset_query(set = 'Первый период', fill = '#244e81'),
                     upset_query(set = 'Второй период', fill = '#19868f'), 
                     upset_query(set = 'Третий период', fill = '#bcd0e5'), 
                     upset_query(set = 'Четвертый период', fill = '#d3bb9c')), 
      matrix=(intersection_matrix(geom=geom_point(shape='circle filled', size=3)) + 
                scale_color_manual(values=c('Первый период'='#244e81', 'Второй период'='#19868f',
                                            'Третий период'='#bcd0e5', 'Четвертый период'='#d3bb9c')))) 
s??plot_theme

library(ggplot2)
library(ComplexUpset)
library(ggpubr)
# Преобразование графика в объект ggplot2

# Сохранение графика в SVG
ggsave("my_plot_tg.svg", plot = grap, device = "svg")


#### загрузим для 4 периодов ####
library(reshape)   
library(readr)
library(readxl)
#обратимся к папке и достанем оттуда файлы 
setwd("/Users/macbook/Desktop/Доходность с топиками телеграм 4 периода")
file.list <- list.files(pattern='*.xlsx')
df.list <- lapply(file.list, as.data.frame(read_excel))
View(df.list)

#### AR для 4 периодов ####
f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data17 <- NULL
data11 <- NULL
data18 <- NULL
data19 <- NULL
data20 <- NULL
#цикл для AR процесса
for(i in 1:193) {
  cat("Итерация номер", i, "\n")
  
  #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,c(1:11)]
  #изменяем все имена
  names(data_comp) <- gsub("value.", "", names(data_comp))
  
  #отфильтруем даты
  data_comp <- data_comp[order(as.Date(data_comp$data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$data))
  data_comp$data <- as.Date(data_comp$data)
  names(interfax_df_int)[1] <- 'data'
  
  #объединим словари и доходности
  data_comp <- left_join(interfax_df_int[,1:31],data_comp, by = 'data')
  
  
  #расставим 0 и 1
  data_comp$ind <- -(is.na(data_comp$price_close)*1 - 1)
  
  #найдем двойные пропуски
  data_comp$ind2 <- 0
  for(j in 2:(dim(data_comp)[1]-1)) {
    data_comp$ind2[j] <- (data_comp$ind[j] == 0 & data_comp$ind[j-1]*data_comp$ind[j+1] == 0)*1
  }
  
  #заполним средним
  for(l in 2:31){
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    l_for_names <- as.data.frame(l)
    a <- as.character(l_for_names[1,1]-1)
    #слово топик для названия
    b <- 'mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- data_comp[,l]
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a, sep='_')
  }
  
  #data_comp$d_mean <- data_comp$d
  names(data_comp)
  for(k in 2:(dim(data_comp)[1]-1)) {
    for(p in 44:73) {
      data_comp[k,p] <- ifelse(data_comp$ind2[k] == 1, (data_comp$ind2[k-1]*data_comp[k-1,p] + 
                                                          data_comp$ind2[k]*data_comp[k,p] + 
                                                          data_comp$ind2[k+1]*data_comp[k+1,p])/2, data_comp[k,p])
    }
  }
  
  data_comp <- data_comp[,-c(2:31, 42,43)]
  
  #сделаем лаги топиков
  
  for(s in 12:41) {
    #зададим исходную размерность
    dim_1 <- as.data.frame(dim(data_comp))
    #достанем номер топика
    s_for_names <- as.data.frame(s)
    a <- as.character(s_for_names[1,1]-11)
    #слово топик для названия
    b <- 'mean_topic'
    
    #сделаем первый лаг и введем название 
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],1)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'1', sep='_')
    
    #сделаем второй лаг и название 
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],2)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'2', sep='_')
    
    #и так далее
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],3)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'3', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],4)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'4', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],5)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'5', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],6)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'6', sep='_')
    
    dim_1 <- as.data.frame(dim(data_comp))
    data_comp[,(dim_1[2,1]+1)] <- lag(data_comp[,s],7)
    names(data_comp)[dim_1[2,1]+1] <- paste(b,a,'7', sep='_')
  }
  
  names(data_comp)
  data_comp <- na.omit(data_comp)
  
  data_comp[-1,(dim(data_comp)[2]+1)] <- diff(as.numeric(data_comp$price_close))/na.omit(dplyr::lag(as.numeric(data_comp$price_close), 1))
  names(data_comp)[252] <- 'yield'
  data_comp$yield <- as.numeric(data_comp$yield)
  
  #лаги доходности
  data_comp$y_lag_1 <- lag(data_comp$yield, 1)
  data_comp$y_lag_2 <- lag(data_comp$yield, 2)
  data_comp$y_lag_3 <- lag(data_comp$yield, 3)
  data_comp$y_lag_4 <- lag(data_comp$yield, 4)
  data_comp$y_lag_5 <- lag(data_comp$yield, 5)
  data_comp$y_lag_6 <- lag(data_comp$yield, 6)
  data_comp$y_lag_7 <- lag(data_comp$yield, 7)
  
  #установка диапазона дат
  start_date <- as.Date("2022-02-25")
  end_date <- as.Date("2022-04-03")
  
  # Удаление строк с датами в этом диапазоне
  data_comp <- data_comp[!(data_comp$data >= start_date & data_comp$data <= end_date),]
  
  data_comp <- na.omit(data_comp)
  
  data1 <- data_comp[,c(12:251)]
  data_y_lags <- data_comp[,c(252:259)]
  
  #####
  N <- which(abs(data_comp[,252])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  data_y_lags <- if(length(N)>0) data_y_lags[-N,] else data_y_lags
  f <- c(f,length(N))
  names(data_comp)
  
  data_comp <- as.data.frame(data_comp)
  
  data_comp[,12:259] <- data_comp[,12:259] %>% mutate_all(as.numeric)
  
  
  ##### разделим на 4 периода  ####
  dim_of_data <- as.data.frame(t(as.data.frame(dim(data_comp))))
  names(dim_of_data)[1] <- 'number'
  for_filter <- round(as.numeric(dim_of_data$number/4))
  
  data_comp1 <- data_comp[c(1:for_filter),] %>% as.data.frame()
  data_comp2 <- data_comp[c((for_filter+1):(2*for_filter)),]  %>% as.data.frame()
  data_comp3 <- data_comp[c((2*for_filter+1):(3*for_filter)),]  %>% as.data.frame()
  data_comp4 <- data_comp[c((3*for_filter+1):(dim_of_data$number)),] %>% as.data.frame()
  
  data1_1 <- data1[c(1:for_filter),] %>% as.data.frame()
  data1_2 <- data1[c((for_filter+1):(2*for_filter)),] %>% as.data.frame()
  data1_3 <- data1[c((2*for_filter+1):(3*for_filter)),] %>% as.data.frame()
  data1_4 <- data1[c((3*for_filter+1):(dim_of_data$number)),] %>% as.data.frame()
  
  data_y_lags1 <- data_y_lags[c(1:for_filter),] %>% as.data.frame()
  data_y_lags2 <- data_y_lags[c((for_filter+1):(2*for_filter)),] %>% as.data.frame()
  data_y_lags3 <- data_y_lags[c((2*for_filter+1):(3*for_filter)),] %>% as.data.frame()
  data_y_lags4 <- data_y_lags[c((3*for_filter+1):(dim_of_data$number)),] %>% as.data.frame()
  
  
  #### лассо для 1 периода ####
  mod1 <- glmnet(data1_1,data_comp1[,252], 
                 alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path1 <- as.data.frame(glmnetPath(mod1)$leave)
  a1 <- max(glmnetPath(mod1)$leave$ord)-2
  b1 <- max(glmnetPath(mod1)$leave$ord)
  vec1 <- as.data.frame(a1:b1)
  names(vec1)[1] <- 'ord'
  
  #отберем значимые топики
  path1 <- left_join(vec1,path1,by='ord')
  top1 <- c(path1$var)
  
  #отберем топики
  reg1 <- data1_1 %>% dplyr::select(top1)
  
  
  #### лассо для 2 периода ####
  mod2 <- glmnet(data1_2,data_comp2[,252], 
                 alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path2 <- as.data.frame(glmnetPath(mod2)$leave)
  a2 <- max(glmnetPath(mod2)$leave$ord)-2
  b2 <- max(glmnetPath(mod2)$leave$ord)
  vec2 <- as.data.frame(a2:b2)
  names(vec2)[1] <- 'ord'
  
  #отберем значимые топики
  path2 <- left_join(vec2,path2,by='ord')
  top2 <- c(path2$var)
  
  #отберем топики
  reg2 <- data1_2 %>% dplyr::select(top2)
  
  
  #### лассо для 3 периода ####
  mod3 <- glmnet(data1_3,data_comp3[,252], 
                 alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path3 <- as.data.frame(glmnetPath(mod3)$leave)
  a3 <- max(glmnetPath(mod3)$leave$ord)-2
  b3 <- max(glmnetPath(mod3)$leave$ord)
  vec3 <- as.data.frame(a3:b3)
  names(vec3)[1] <- 'ord'
  
  #отберем значимые топики
  path3 <- left_join(vec3,path3,by='ord')
  top3 <- c(path3$var)
  
  #отберем топики
  reg3 <- data1_3 %>% dplyr::select(top3)
  
  
  #### лассо для 4 периода ####
  mod4 <- glmnet(data1_4,data_comp4[,252], 
                 alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path4 <- as.data.frame(glmnetPath(mod4)$leave)
  a4 <- max(glmnetPath(mod4)$leave$ord)-2
  b4 <- max(glmnetPath(mod4)$leave$ord)
  vec4 <- as.data.frame(a4:b4)
  names(vec4)[1] <- 'ord'
  
  #отберем значимые топики
  path4 <- left_join(vec4,path4,by='ord')
  top4 <- c(path4$var)
  
  #отберем топики
  reg4 <- data1_4 %>% dplyr::select(top4)
  
  
  #1 период
  #только по топикам
  set.seed(789)
  mod1_1 <- train(yield~.,
                  data=data_y_lags1, method = "lm",
                  trControl = trainControl(method = 'repeatedcv',
                                           number = 10, repeats = 10))
  
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  all_1 <- cbind(data_y_lags1, reg1)
  mod1_2 <- caret::train(yield ~ .,data=all_1, 
                         method = "lm",
                         trControl = trainControl(method = 'repeatedcv',
                                                  number = 10, repeats = 10))
  
  #2 период
  #только по топикам
  set.seed(789)
  mod2_1 <- train(yield~.,
                  data=data_y_lags2, method = "lm",
                  trControl = trainControl(method = 'repeatedcv',
                                           number = 10, repeats = 10))
  
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  all_2 <- cbind(data_y_lags2, reg2)
  mod2_2 <- caret::train(yield ~ .,data=all_2, 
                         method = "lm",
                         trControl = trainControl(method = 'repeatedcv',
                                                  number = 10, repeats = 10))
  #3 период
  #только по топикам
  set.seed(789)
  mo3_1 <- train(yield~.,
                 data=data_y_lags3, method = "lm",
                 trControl = trainControl(method = 'repeatedcv',
                                          number = 10, repeats = 10))
  
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  all_3 <- cbind(data_y_lags3, reg3)
  mod3_2 <- caret::train(yield ~ .,data=all_3, 
                         method = "lm",
                         trControl = trainControl(method = 'repeatedcv',
                                                  number = 10, repeats =10))
  
  #4 период
  #только по топикам
  set.seed(789)
  mo4_1 <- train(yield~.,
                 data=data_y_lags4, method = "lm",
                 trControl = trainControl(method = 'repeatedcv',
                                          number = 10, repeats = 10))
  
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  all_4 <- cbind(data_y_lags4, reg4)
  mod4_2 <- caret::train(yield ~ .,data=all_4, 
                         method = "lm",
                         trControl = trainControl(method = 'repeatedcv',
                                                  number = 10, repeats = 10))
  
  
  t1 <- t.test(mod1_1$resample$Rsquared,mod1_2$resample$Rsquared)
  r2_no_news1 <-  summary(mod1_1)$adj.r.squared
  r2_with_news1 <- summary(mod1_2)$adj.r.squared
  delta1 <- r2_with_news1 - r2_no_news1
  
  t2 <- t.test(mod2_1$resample$Rsquared,mod2_2$resample$Rsquared)
  r2_no_news2 <-  summary(mod2_1)$adj.r.squared
  r2_with_news2 <- summary(mod2_2)$adj.r.squared
  delta2 <- r2_with_news2 - r2_no_news2
  
  t3 <- t.test(mo3_1$resample$Rsquared,mod3_2$resample$Rsquared)
  r2_no_news3 <-  summary(mo3_1)$adj.r.squared
  r2_with_news3 <- summary(mod3_2)$adj.r.squared
  delta3 <- r2_with_news3 - r2_no_news3
  
  t4 <- t.test(mo4_1$resample$Rsquared,mod4_2$resample$Rsquared)
  r2_no_news4 <-  summary(mo4_1)$adj.r.squared
  r2_with_news4 <- summary(mod4_2)$adj.r.squared
  delta4 <- r2_with_news4 - r2_no_news4
  
  period <- as.data.frame(for_filter)
  names(period)[1] <- 'Длина одного периода'
  
  #достанем значимые лаги 1 период
  table_full_AR_3 <- as.data.frame(summary(mod1_2)$coefficients)
  names(table_full_AR_3)[4] <- 'P-value'
  g3 <- grep('mean_topic', rownames(table_full_AR_3))
  table3 <- as.data.frame(table_full_AR_3[g3,])
  table_full_AR_3 <- table3 %>% dplyr::filter(table3[,4] < 0.1)
  table_full_AR_3_2 <- as.data.frame(rownames(table_full_AR_3))
  table_full_AR_3_1 <- if(dim(table_full_AR_3)[1]==0) table_full_AR_3 else cbind(table_full_AR_3_2,table_full_AR_3)
  table_full_AR_3_1 <- if(dim(table_full_AR_3)[1]==0) table_full_AR_3 else table_full_AR_3_1 %>% mutate(ticker=list_comp[i])
  data17 <- rbind(data17,table_full_AR_3_1)
  
  #достанем значимые лаги 2 период
  table_full_AR <- as.data.frame(summary(mod2_2)$coefficients)
  names(table_full_AR)[4] <- 'P-value'
  g <- grep('mean_topic', rownames(table_full_AR))
  table <- as.data.frame(table_full_AR[g,])
  table_full_AR <- table %>% dplyr::filter(table[,4] < 0.1)
  table_full_AR_2 <- as.data.frame(rownames(table_full_AR))
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR else cbind(table_full_AR_2,table_full_AR)
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR_1 else table_full_AR_1 %>% mutate(ticker=list_comp[i])
  data18 <- rbind(data18,table_full_AR_1)
  
  
  #достанем значимые лаги 3 период
  table_full_AR_3 <- as.data.frame(summary(mod3_2)$coefficients)
  names(table_full_AR_3)[4] <- 'P-value'
  g3 <- grep('mean_topic', rownames(table_full_AR_3))
  table3 <- as.data.frame(table_full_AR_3[g3,])
  table_full_AR_3 <- table3 %>% dplyr::filter(table3[,4] < 0.1)
  table_full_AR_3_2 <- as.data.frame(rownames(table_full_AR_3))
  table_full_AR_3_1 <- if(dim(table_full_AR_3)[1]==0) table_full_AR_3 else cbind(table_full_AR_3_2,table_full_AR_3)
  table_full_AR_3_1 <- if(dim(table_full_AR_3)[1]==0) table_full_AR_3 else table_full_AR_3_1 %>% mutate(ticker=list_comp[i])
  data19 <- rbind(data19,table_full_AR_3_1)
  
  #достанем значимые лаги 4 период
  table_full_AR <- as.data.frame(summary(mod4_2)$coefficients)
  names(table_full_AR)[4] <- 'P-value'
  g <- grep('mean_topic', rownames(table_full_AR))
  table <- as.data.frame(table_full_AR[g,])
  table_full_AR <- table %>% dplyr::filter(table[,4] < 0.1)
  table_full_AR_2 <- as.data.frame(rownames(table_full_AR))
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR else cbind(table_full_AR_2,table_full_AR)
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR_1 else table_full_AR_1 %>% mutate(ticker=list_comp[i])
  data20 <- rbind(data20,table_full_AR_1)
  
  data9 <- rbind(data9,c(list_comp[i], 
                         t1$statistic,r2_no_news1,r2_with_news1,delta1,
                         t2$statistic,r2_no_news2,r2_with_news2,delta2,
                         t3$statistic,r2_no_news3,r2_with_news3,delta3,
                         t4$statistic,r2_no_news4,r2_with_news4,delta4,
                         period))}

data17 <- data17 %>% as.data.frame()
data18 <- data18 %>% as.data.frame()
data19 <- data19 %>% as.data.frame()
data20 <- data20 %>% as.data.frame()

data9 <- as.data.frame(data9)
View(data9)



#### сравним качество моделей ####
prop.test(x=c(87,107), n =c(193,193))
prop.test(x=c(87,90), n =c(193,193))
prop.test(x=c(87,120), n =c(193,193))
prop.test(x=c(107,90), n =c(193,193))
prop.test(x=c(107,120), n =c(193,193))
prop.test(x=c(90,120), n =c(193,193))

prop.test(x=c(87,102), n =c(193,193))
prop.test(x=c(112,107), n =c(193,193))
prop.test(x=c(91,90), n =c(193,193))
prop.test(x=c(116,120), n =c(193,193))

dfint <- read_excel("/Users/macbook/Desktop/По периодам/int_свод_AR.xlsx")
dfnyt <- read_excel("/Users/macbook/Desktop/По периодам/nyt_свод_AR.xlsx")

t.test(as.numeric(dfint$V5), as.numeric(dfnyt$V5))
t.test(as.numeric(dfint$V9), as.numeric(dfnyt$V9))
t.test(as.numeric(dfint$V13), as.numeric(dfnyt$V13))
t.test(as.numeric(dfint$V17), as.numeric(dfnyt$V17))

####################### телеграм ##########################
#### LDA ####
#### текстовый анализ ###### ####
#разделим на токены и удалим стоп-слова
final_itog2_tokens <- tokens(
  final_itog2$clear_message, what = "word", remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
  remove_url = TRUE, remove_separators = TRUE, split_hyphens = FALSE, split_tags = FALSE,
  include_docvars = TRUE, padding = FALSE, verbose = quanteda_options("verbose")) %>%
  tokens_remove(pattern = quanteda::stopwords("ru", source = "snowball")) %>%
  tokens_remove(pattern = quanteda::stopwords("ru", source = "stopwords-iso")) %>% 
  tokens_remove(pattern = quanteda::stopwords("ru", source = "marimo")) %>%
  tokens_remove(pattern = quanteda::stopwords("ru", source = "nltk")) %>%
  tokens_tolower()

#удалим пунктуацию и цифры и превратим в DFM
final_itog2_tokens <- tokens_remove(final_itog2_tokens, pattern = c(stopwords("ru"), "*-time", "updated-*", 
                                                                    "gmt", "bst"))

final_itog2_tokens <- tokens_wordstem(final_itog2_tokens, language = "ru") 


dfmat_final_itog2_tokens <- dfm(final_itog2_tokens) %>% 
  dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile",
           max_docfreq = 0.1, docfreq_type = "prop")

#определим количество словарей
library("ldatuning")
library("topicmodels")

#работает 8-10 часов 
result <- FindTopicsNumber(
  dfmat_final_itog2_tokens,
  topics = seq(from = 5, to = 25, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)
FindTopicsNumber_plot(result)

#латентное размещение Дирихле
library(seededlda)
library(lubridate)
set.seed(1234)
lda_telegram_25 <- textmodel_lda(dfmat_final_itog2_tokens, k = 25)

terms(lda_telegram_25, 22)

data <- as.data.frame(terms(lda_telegram_25, 22))
View(data)
library(writexl)
write_xlsx(data, 'топики_для_тг.xlsx')

#по топикам
topics_tel <- lda_telegram_25$theta

#Прицепили это к изначальному датасету - теперь есть пара новость и то, вероятности по топикам 
tel_itog_topics <- cbind(final_itog2, topics_tel)
View(tel_itog_topics)
names(tel_itog_topics)
names(tel_itog_topics_fin)[1] <- 'data'

tel_itog_topics_fin <- tel_itog_topics %>% group_by(date) %>% 
  summarise(topik_1 = mean(topic1), 
            topik_2 = mean(topic2), 
            topik_3 = mean(topic3), 
            topik_4 = mean(topic4),
            topik_5 = mean(topic5), 
            topik_6 = mean(topic6), 
            topik_7 = mean(topic7), 
            topik_8 = mean(topic8),
            topik_9 = mean(topic9), 
            topik_10 = mean(topic10),
            topik_11 = mean(topic11), 
            topik_12 = mean(topic12), 
            topik_13 = mean(topic13), 
            topik_14 = mean(topic14),
            topik_15 = mean(topic15), 
            topik_16 = mean(topic16), 
            topik_17 = mean(topic17), 
            topik_18 = mean(topic18),
            topik_19 = mean(topic19), 
            topik_20 = mean(topic20),
            topik_21 = mean(topic21),
            topik_22 = mean(topic22),
            topik_23 = mean(topic23),
            topik_24 = mean(topic24),
            topik_25 = mean(topic25),
            .groups = 'drop') %>%
  as.data.frame()


#### Добавить топики к компании ####
ABIO_ok<- left_join(ABIO,tel_itog_topics_fin,by="data")
ABRD_ok<- left_join(ABRD,tel_itog_topics_fin,by="data")
ACKO_ok<- left_join(ACKO,tel_itog_topics_fin,by="data")
AFKS_ok<- left_join(AFKS,tel_itog_topics_fin,by="data")
AFLT_ok<- left_join(AFLT,tel_itog_topics_fin,by="data")
AGRODR_ok<- left_join(AGRODR,tel_itog_topics_fin,by="data")
AKRN_ok<- left_join(AKRN,tel_itog_topics_fin,by="data")
ALRS_ok<- left_join(ALRS,tel_itog_topics_fin,by="data")
AMEZ_ok<- left_join(AMEZ,tel_itog_topics_fin,by="data")
APTK_ok<- left_join(APTK,tel_itog_topics_fin,by="data")
AQUA_ok<- left_join(AQUA,tel_itog_topics_fin,by="data")
ARSA_ok<- left_join(ARSA,tel_itog_topics_fin,by="data")
ASSB_ok<- left_join(ASSB,tel_itog_topics_fin,by="data")
AVAN_ok<- left_join(AVAN,tel_itog_topics_fin,by="data")
BANE_ok<- left_join(BANE,tel_itog_topics_fin,by="data")
BELU_ok<- left_join(BELU,tel_itog_topics_fin,by="data")
BLNG_ok<- left_join(BLNG,tel_itog_topics_fin,by="data")
BRZL_ok<- left_join(BRZL,tel_itog_topics_fin,by="data")
BSPB_ok<- left_join(BSPB,tel_itog_topics_fin,by="data")
CARM_ok<- left_join(CARM,tel_itog_topics_fin,by="data")
CBOM_ok<- left_join(CBOM,tel_itog_topics_fin,by="data")
CHEP_ok<- left_join(CHEP,tel_itog_topics_fin,by="data")
CHGZ_ok<- left_join(CHGZ,tel_itog_topics_fin,by="data")
CHKZ_ok<- left_join(CHKZ,tel_itog_topics_fin,by="data")
CHMF_ok<- left_join(CHMF,tel_itog_topics_fin,by="data")
CHMK_ok<- left_join(CHMK,tel_itog_topics_fin,by="data")
CIANDR_ok<- left_join(CIANDR,tel_itog_topics_fin,by="data")
CNTL_ok<- left_join(CNTL,tel_itog_topics_fin,by="data")
DASB_ok<- left_join(DASB,tel_itog_topics_fin,by="data")
DIOD_ok<- left_join(DIOD,tel_itog_topics_fin,by="data")
DSKY_ok<- left_join(DSKY,tel_itog_topics_fin,by="data")
DVEC_ok<- left_join(DVEC,tel_itog_topics_fin,by="data")
DZRD_ok<- left_join(DZRD,tel_itog_topics_fin,by="data")
EELT_ok<- left_join(EELT,tel_itog_topics_fin,by="data")
ELFV_ok<- left_join(ELFV,tel_itog_topics_fin,by="data")
ELTZ_ok<- left_join(ELTZ,tel_itog_topics_fin,by="data")
ENPG_ok<- left_join(ENPG,tel_itog_topics_fin,by="data")
ETLNDR_ok<- left_join(ETLNDR,tel_itog_topics_fin,by="data")
FEES_ok<- left_join(FEES,tel_itog_topics_fin,by="data")
FESH_ok<- left_join(FESH,tel_itog_topics_fin,by="data")
FIVEDR_ok<- left_join(FIVEDR,tel_itog_topics_fin,by="data")
FIXPDR_ok<- left_join(FIXPDR,tel_itog_topics_fin,by="data")
FLOT_ok<- left_join(FLOT,tel_itog_topics_fin,by="data")
FORTUM_ok<- left_join(FORTUM,tel_itog_topics_fin,by="data")
GAZA_ok<- left_join(GAZA,tel_itog_topics_fin,by="data")
GAZP_ok<- left_join(GAZP,tel_itog_topics_fin,by="data")
GCHE_ok<- left_join(GCHE,tel_itog_topics_fin,by="data")
GECO_ok<- left_join(GECO,tel_itog_topics_fin,by="data")
GEMA_ok<- left_join(GEMA,tel_itog_topics_fin,by="data")
GEMCDR_ok<- left_join(GEMCDR,tel_itog_topics_fin,by="data")
GLTRDR_ok<- left_join(GLTRDR,tel_itog_topics_fin,by="data")
GMKN_ok<- left_join(GMKN,tel_itog_topics_fin,by="data")
GTRK_ok<- left_join(GTRK,tel_itog_topics_fin,by="data")
HHRUDR_ok<- left_join(HHRUDR,tel_itog_topics_fin,by="data")
HMSGDR_ok<- left_join(HMSGDR,tel_itog_topics_fin,by="data")
HYDR_ok<- left_join(HYDR,tel_itog_topics_fin,by="data")
IGST_ok<- left_join(IGST,tel_itog_topics_fin,by="data")
INGR_ok<- left_join(INGR,tel_itog_topics_fin,by="data")
IRAO_ok<- left_join(IRAO,tel_itog_topics_fin,by="data")
IRGZ_ok<- left_join(IRGZ,tel_itog_topics_fin,by="data")
IRKT_ok<- left_join(IRKT,tel_itog_topics_fin,by="data")
JNOS_ok<- left_join(JNOS,tel_itog_topics_fin,by="data")
KAZT_ok<- left_join(KAZT,tel_itog_topics_fin,by="data")
KBSB_ok<- left_join(KBSB,tel_itog_topics_fin,by="data")
KCHE_ok<- left_join(KCHE,tel_itog_topics_fin,by="data")
KGKC_ok<- left_join(KGKC,tel_itog_topics_fin,by="data")
KLSB_ok<- left_join(KLSB,tel_itog_topics_fin,by="data")
KMAZ_ok<- left_join(KMAZ,tel_itog_topics_fin,by="data")
KMEZ_ok<- left_join(KMEZ,tel_itog_topics_fin,by="data")
KOGK_ok<- left_join(KOGK,tel_itog_topics_fin,by="data")
KRKN_ok<- left_join(KRKN,tel_itog_topics_fin,by="data")
KROT_ok<- left_join(KROT,tel_itog_topics_fin,by="data")
KRSB_ok<- left_join(KRSB,tel_itog_topics_fin,by="data")
KTSB_ok<- left_join(KTSB,tel_itog_topics_fin,by="data")
KUBE_ok<- left_join(KUBE,tel_itog_topics_fin,by="data")
KUZB_ok<- left_join(KUZB,tel_itog_topics_fin,by="data")
KZOS_ok<- left_join(KZOS,tel_itog_topics_fin,by="data")
LENT_ok<- left_join(LENT,tel_itog_topics_fin,by="data")
LIFE_ok<- left_join(LIFE,tel_itog_topics_fin,by="data")
LKOH_ok<- left_join(LKOH,tel_itog_topics_fin,by="data")
LNZL_ok<- left_join(LNZL,tel_itog_topics_fin,by="data")
LPSB_ok<- left_join(LPSB,tel_itog_topics_fin,by="data")
LSNG_ok<- left_join(LSNG,tel_itog_topics_fin,by="data")
LSRG_ok<- left_join(LSRG,tel_itog_topics_fin,by="data")
LVHK_ok<- left_join(LVHK,tel_itog_topics_fin,by="data")
MAGE_ok<- left_join(MAGE,tel_itog_topics_fin,by="data")
MAGN_ok<- left_join(MAGN,tel_itog_topics_fin,by="data")
MDMGDR_ok<- left_join(MDMGDR,tel_itog_topics_fin,by="data")
MERF_ok<- left_join(MERF,tel_itog_topics_fin,by="data")
MFGS_ok<- left_join(MFGS,tel_itog_topics_fin,by="data")
MGNT_ok<- left_join(MGNT,tel_itog_topics_fin,by="data")
MGNZ_ok<- left_join(MGNZ,tel_itog_topics_fin,by="data")
MGTS_ok<- left_join(MGTS,tel_itog_topics_fin,by="data")
MISB_ok<- left_join(MISB,tel_itog_topics_fin,by="data")
MOEX_ok<- left_join(MOEX,tel_itog_topics_fin,by="data")
MRKC_ok<- left_join(MRKC,tel_itog_topics_fin,by="data")
MRKK_ok<- left_join(MRKK,tel_itog_topics_fin,by="data")
MRKP_ok<- left_join(MRKP,tel_itog_topics_fin,by="data")
MRKS_ok<- left_join(MRKS,tel_itog_topics_fin,by="data")
MRKU_ok<- left_join(MRKU,tel_itog_topics_fin,by="data")
MRKV_ok<- left_join(MRKV,tel_itog_topics_fin,by="data")
MRKY_ok<- left_join(MRKY,tel_itog_topics_fin,by="data")
MRKZ_ok<- left_join(MRKZ,tel_itog_topics_fin,by="data")
MRSB_ok<- left_join(MRSB,tel_itog_topics_fin,by="data")
MSNG_ok<- left_join(MSNG,tel_itog_topics_fin,by="data")
MSRS_ok<- left_join(MSRS,tel_itog_topics_fin,by="data")
MSTT_ok<- left_join(MSTT,tel_itog_topics_fin,by="data")
MTLR_ok<- left_join(MTLR,tel_itog_topics_fin,by="data")
MTSS_ok<- left_join(MTSS,tel_itog_topics_fin,by="data")
MVID_ok<- left_join(MVID,tel_itog_topics_fin,by="data")
NAUK_ok<- left_join(NAUK,tel_itog_topics_fin,by="data")
NFAZ_ok<- left_join(NFAZ,tel_itog_topics_fin,by="data")
NKHP_ok<- left_join(NKHP,tel_itog_topics_fin,by="data")
NKNC_ok<- left_join(NKNC,tel_itog_topics_fin,by="data")
NKSH_ok<- left_join(NKSH,tel_itog_topics_fin,by="data")
NLMK_ok<- left_join(NLMK,tel_itog_topics_fin,by="data")
NMTP_ok<- left_join(NMTP,tel_itog_topics_fin,by="data")
NNSB_ok<- left_join(NNSB,tel_itog_topics_fin,by="data")
NSVZ_ok<- left_join(NSVZ,tel_itog_topics_fin,by="data")
NVTK_ok<- left_join(NVTK,tel_itog_topics_fin,by="data")
ODVA_ok<- left_join(ODVA,tel_itog_topics_fin,by="data")
OGKB_ok<- left_join(OGKB,tel_itog_topics_fin,by="data")
OKEYDR_ok<- left_join(OKEYDR,tel_itog_topics_fin,by="data")
OZONDR_ok<- left_join(OZONDR,tel_itog_topics_fin,by="data")
PAZA_ok<- left_join(PAZA,tel_itog_topics_fin,by="data")
PHOR_ok<- left_join(PHOR,tel_itog_topics_fin,by="data")
PIKK_ok<- left_join(PIKK,tel_itog_topics_fin,by="data")
PLZL_ok<- left_join(PLZL,tel_itog_topics_fin,by="data")
PMSB_ok<- left_join(PMSB,tel_itog_topics_fin,by="data")
POLY_ok<- left_join(POLY,tel_itog_topics_fin,by="data")
POSI_ok<- left_join(POSI,tel_itog_topics_fin,by="data")
PRFN_ok<- left_join(PRFN,tel_itog_topics_fin,by="data")
PRMB_ok<- left_join(PRMB,tel_itog_topics_fin,by="data")
QIWIDR_ok<- left_join(QIWIDR,tel_itog_topics_fin,by="data")
RASP_ok<- left_join(RASP,tel_itog_topics_fin,by="data")
RBCM_ok<- left_join(RBCM,tel_itog_topics_fin,by="data")
RDRB_ok<- left_join(RDRB,tel_itog_topics_fin,by="data")
RENI_ok<- left_join(RENI,tel_itog_topics_fin,by="data")
RGSS_ok<- left_join(RGSS,tel_itog_topics_fin,by="data")
RKKE_ok<- left_join(RKKE,tel_itog_topics_fin,by="data")
RNFT_ok<- left_join(RNFT,tel_itog_topics_fin,by="data")
ROLO_ok<- left_join(ROLO,tel_itog_topics_fin,by="data")
ROSB_ok<- left_join(ROSB,tel_itog_topics_fin,by="data")
ROSN_ok<- left_join(ROSN,tel_itog_topics_fin,by="data")
ROST_ok<- left_join(ROST,tel_itog_topics_fin,by="data")
RTGZ_ok<- left_join(RTGZ,tel_itog_topics_fin,by="data")
RTKM_ok<- left_join(RTKM,tel_itog_topics_fin,by="data")
RTSB_ok<- left_join(RTSB,tel_itog_topics_fin,by="data")
RUAL_ok<- left_join(RUAL,tel_itog_topics_fin,by="data")
RUGR_ok<- left_join(RUGR,tel_itog_topics_fin,by="data")
RUSI_ok<- left_join(RUSI,tel_itog_topics_fin,by="data")
RZSB_ok<- left_join(RZSB,tel_itog_topics_fin,by="data")
SAGO_ok<- left_join(SAGO,tel_itog_topics_fin,by="data")
SARE_ok<- left_join(SARE,tel_itog_topics_fin,by="data")
SBER_ok<- left_join(SBER,tel_itog_topics_fin,by="data")
SELG_ok<- left_join(SELG,tel_itog_topics_fin,by="data")
SFIN_ok<- left_join(SFIN,tel_itog_topics_fin,by="data")
SGZH_ok<- left_join(SGZH,tel_itog_topics_fin,by="data")
SIBN_ok<- left_join(SIBN,tel_itog_topics_fin,by="data")
SLEN_ok<- left_join(SLEN,tel_itog_topics_fin,by="data")
SMLT_ok<- left_join(SMLT,tel_itog_topics_fin,by="data")
SNGS_ok<- left_join(SNGS,tel_itog_topics_fin,by="data")
SOFL_ok<- left_join(SOFL,tel_itog_topics_fin,by="data")
SPBE_ok<- left_join(SPBE,tel_itog_topics_fin,by="data")
STSB_ok<- left_join(STSB,tel_itog_topics_fin,by="data")
SVAV_ok<- left_join(SVAV,tel_itog_topics_fin,by="data")
SVET_ok<- left_join(SVET,tel_itog_topics_fin,by="data")
TASB_ok<- left_join(TASB,tel_itog_topics_fin,by="data")
TATN_ok<- left_join(TATN,tel_itog_topics_fin,by="data")
TCSGDR_ok<- left_join(TCSGDR,tel_itog_topics_fin,by="data")
TGKA_ok<- left_join(TGKA,tel_itog_topics_fin,by="data")
TGKB_ok<- left_join(TGKB,tel_itog_topics_fin,by="data")
TGKD_ok<- left_join(TGKD,tel_itog_topics_fin,by="data")
TGKN_ok<- left_join(TGKN,tel_itog_topics_fin,by="data")
TNSE_ok<- left_join(TNSE,tel_itog_topics_fin,by="data")
TORS_ok<- left_join(TORS,tel_itog_topics_fin,by="data")
TRMK_ok<- left_join(TRMK,tel_itog_topics_fin,by="data")
TTLK_ok<- left_join(TTLK,tel_itog_topics_fin,by="data")
TUZA_ok<- left_join(TUZA,tel_itog_topics_fin,by="data")
UKUZ_ok<- left_join(UKUZ,tel_itog_topics_fin,by="data")
UNAC_ok<- left_join(UNAC,tel_itog_topics_fin,by="data")
UNKL_ok<- left_join(UNKL,tel_itog_topics_fin,by="data")
UPRO_ok<- left_join(UPRO,tel_itog_topics_fin,by="data")
URKZ_ok<- left_join(URKZ,tel_itog_topics_fin,by="data")
USBN_ok<- left_join(USBN,tel_itog_topics_fin,by="data")
UTAR_ok<- left_join(UTAR,tel_itog_topics_fin,by="data")
UWGN_ok<- left_join(UWGN,tel_itog_topics_fin,by="data")
VEON_RX_ok<- left_join(VEON_RX,tel_itog_topics_fin,by="data")
VGSB_ok<- left_join(VGSB,tel_itog_topics_fin,by="data")
VJGZ_ok<- left_join(VJGZ,tel_itog_topics_fin,by="data")
VKCO_ok<- left_join(VKCO,tel_itog_topics_fin,by="data")
VLHZ_ok<- left_join(VLHZ,tel_itog_topics_fin,by="data")
VRSB_ok<- left_join(VRSB,tel_itog_topics_fin,by="data")
VSMO_ok<- left_join(VSMO,tel_itog_topics_fin,by="data")
VSYD_ok<- left_join(VSYD,tel_itog_topics_fin,by="data")
VTBR_ok<- left_join(VTBR,tel_itog_topics_fin,by="data")
WTCM_ok<- left_join(WTCM,tel_itog_topics_fin,by="data")
WUSH_ok<- left_join(WUSH,tel_itog_topics_fin,by="data")
YAKG_ok<- left_join(YAKG,tel_itog_topics_fin,by="data")
YKEN_ok<- left_join(YKEN,tel_itog_topics_fin,by="data")
YNDX_ok<- left_join(YNDX,tel_itog_topics_fin,by="data")
YRSB_ok<- left_join(YRSB,tel_itog_topics_fin,by="data")
ZILL_ok<- left_join(ZILL,tel_itog_topics_fin,by="data")
ZVEZ_ok<- left_join(ZVEZ,tel_itog_topics_fin,by="data")
#### Загрузим доходность с топиками ####
library(reshape)   
library(readr)
library(readxl)
#обратимся к папке и достанем оттуда файлы 
setwd("/Users/macbook/Desktop/Диплом/Доходность с топиками телеграм")
file.list <- list.files(pattern='*.xlsx')
df.list <- lapply(file.list, as.data.frame(read_excel))
View(df.list)



#####пакеты####
library(dplyr)
library(glmnet)
library(solzy)
library(caret)
library(MASS)
library(randomForest)
#### Загрузим список компаний ####
list_comp <- c("ABIO",
               "ABRD",
               "ACKO",
               "AFKS",
               "AFLT",
               "AGRODR",
               "AKRN",
               "ALRS",
               "AMEZ",
               "APTK",
               "AQUA",
               "ARSA",
               "ASSB",
               "AVAN",
               "BANE",
               "BELU",
               "BLNG",
               "BRZL",
               "BSPB",
               "CBOM",
               "CHGZ",
               "CHKZ",
               "CHMF",
               "CHMK",
               "CIANDR",
               "CNTL",
               "DASB",
               "DIOD",
               "DSKY",
               "DVEC",
               "DZRD",
               "EELT",
               "ELFV",
               "ELTZ",
               "ENPG",
               "ETLNDR",
               "FEES",
               "FESH",
               "FIVEDR",
               "FIXPDR",
               "FLOT",
               "FORTUM",
               "GAZA",
               "GAZP",
               "GCHE",
               "GECO",
               "GEMA",
               "GEMCDR",
               "GLTRDR",
               "GMKN",
               "GTRK",
               "HHRUDR",
               "HMSGDR",
               "HYDR",
               "IGST",
               "INGR",
               "IRAO",
               "IRGZ",
               "IRKT",
               "JNOS",
               "KAZT",
               "KBSB",
               "KCHE",
               "KGKC",
               "KLSB",
               "KMAZ",
               "KMEZ",
               "KOGK",
               "KRKN",
               "KROT",
               "KRSB",
               "KTSB",
               "KUBE",
               "KUZB",
               "KZOS",
               "LENT",
               "LIFE",
               "LKOH",
               "LNZL",
               "LPSB",
               "LSNG",
               "LSRG",
               "LVHK",
               "MAGE",
               "MAGN",
               "MDMGDR",
               "MERF",
               "MFGS",
               "MGNT",
               "MGNZ",
               "MGTS",
               "MISB",
               "MOEX",
               "MRKC",
               "MRKK",
               "MRKP",
               "MRKS",
               "MRKU",
               "MRKV",
               "MRKY",
               "MRKZ",
               "MRSB",
               "MSNG",
               "MSRS",
               "MSTT",
               "MTLR",
               "MTSS",
               "MVID",
               "NAUK",
               "NFAZ",
               "NKHP",
               "NKNC",
               "NKSH",
               "NLMK",
               "NMTP",
               "NNSB",
               "NSVZ",
               "NVTK",
               "ODVA",
               "OGKB",
               "OKEYDR",
               "OZONDR",
               "PAZA",
               "PHOR",
               "PIKK",
               "PLZL",
               "PMSB",
               "POLY",
               "POSI",
               "PRFN",
               "PRMB",
               "QIWIDR",
               "RASP",
               "RBCM",
               "RDRB",
               "RENI",
               "RGSS",
               "RKKE",
               "RNFT",
               "ROLO",
               "ROSB",
               "ROSN",
               "ROST",
               "RTGZ",
               "RTKM",
               "RTSB",
               "RUAL",
               "RUGR",
               "RUSI",
               "RZSB",
               "SAGO",
               "SARE",
               "SBER",
               "SELG",
               "SFIN",
               "SGZH",
               "SIBN",
               "SLEN",
               "SMLT",
               "SNGS",
               "SPBE",
               "STSB",
               "SVET",
               "TASB",
               "TATN",
               "TCSGDR",
               "TGKA",
               "TGKB",
               "TGKD",
               "TGKN",
               "TNSE",
               "TORS",
               "TRMK",
               "TTLK",
               "TUZA",
               "UKUZ",
               "UNAC",
               "UNKL",
               "UPRO",
               "URKZ",
               "USBN",
               "UTAR",
               "UWGN",
               "VEON_RX",
               "VGSB",
               "VJGZ",
               "VKCO",
               "VLHZ",
               "VRSB",
               "VSMO",
               "VSYD",
               "VTBR",
               "WTCM",
               "WUSH",
               "YAKG",
               "YKEN",
               "YNDX",
               "YRSB",
               "ZILL",
               "ZVEZ")



length(list_comp)
#### AR и значимые топики из него ####
f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data11 <- NULL
data13 <- NULL
library(lubridate)

#цикл для AR процесса
for(i in 1:200) {
  cat("Итерация номер", i, "\n")
  #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,c(1:11, 13:17,19,21:22,24:25,27:36)]
  names(data_comp)
  data_comp <- data_comp[order(as.Date(data_comp$value.data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$value.data))
  data_comp$value.data <- as.Date(data_comp$value.data)
  data_comp[,c(2:5, 12:31)] <- data_comp[,c(2:5, 12:31)] %>% mutate_all(as.numeric)
  data_comp[-1,32] <- diff(as.numeric(data_comp[,2]))/na.omit(dplyr::lag(data_comp$value.price, 1))
  names(data_comp)[8] <- 'date'
  data1 <- data_comp[,c(12:31)]
  names(data1)
  #### три лага топиков ####
  
  data1$value.topik_2_1 <- lag(data1$value.topik_2,1)
  data1$value.topik_2_2 <- lag(data1$value.topik_2,2)
  data1$value.topik_2_3 <- lag(data1$value.topik_2,3)
  data1$value.topik_3_1 <- lag(data1$value.topik_3,1)
  data1$value.topik_3_2 <- lag(data1$value.topik_3,2)
  data1$value.topik_3_3 <- lag(data1$value.topik_3,3)
  data1$value.topik_4_1 <- lag(data1$value.topik_4,1)
  data1$value.topik_4_2 <- lag(data1$value.topik_4,2)
  data1$value.topik_4_3 <- lag(data1$value.topik_4,3)
  data1$value.topik_5_1 <- lag(data1$value.topik_5,1)
  data1$value.topik_5_2 <- lag(data1$value.topik_5,2)
  data1$value.topik_5_3 <- lag(data1$value.topik_5,3)
  data1$value.topik_6_1 <- lag(data1$value.topik_6,1)
  data1$value.topik_6_2 <- lag(data1$value.topik_6,2)
  data1$value.topik_6_3 <- lag(data1$value.topik_6,3)
  data1$value.topik_8_1 <- lag(data1$value.topik_8,1)
  data1$value.topik_8_2 <- lag(data1$value.topik_8,2)
  data1$value.topik_8_3 <- lag(data1$value.topik_8,3)
  data1$value.topik_10_1 <- lag(data1$value.topik_10,1)
  data1$value.topik_10_2 <- lag(data1$value.topik_10,2)
  data1$value.topik_10_3 <- lag(data1$value.topik_10,3)
  data1$value.topik_11_1 <- lag(data1$value.topik_11,1)
  data1$value.topik_11_2 <- lag(data1$value.topik_11,2)
  data1$value.topik_11_3 <- lag(data1$value.topik_11,3)
  data1$value.topik_13_1 <- lag(data1$value.topik_13,1)
  data1$value.topik_13_2 <- lag(data1$value.topik_13,2)
  data1$value.topik_13_3 <- lag(data1$value.topik_13,3)
  data1$value.topik_14_1 <- lag(data1$value.topik_14,1)
  data1$value.topik_14_2 <- lag(data1$value.topik_14,2)
  data1$value.topik_14_3 <- lag(data1$value.topik_14,3)
  data1$value.topik_16_1 <- lag(data1$value.topik_16,1)
  data1$value.topik_16_2 <- lag(data1$value.topik_16,2)
  data1$value.topik_16_3 <- lag(data1$value.topik_16,3)
  data1$value.topik_17_1 <- lag(data1$value.topik_17,1)
  data1$value.topik_17_2 <- lag(data1$value.topik_17,2)
  data1$value.topik_17_3 <- lag(data1$value.topik_17,3)
  data1$value.topik_18_1 <- lag(data1$value.topik_18,1)
  data1$value.topik_18_2 <- lag(data1$value.topik_18,2)
  data1$value.topik_18_3 <- lag(data1$value.topik_18,3)
  data1$value.topik_19_1 <- lag(data1$value.topik_19,1)
  data1$value.topik_19_2 <- lag(data1$value.topik_19,2)
  data1$value.topik_19_3 <- lag(data1$value.topik_19,3)
  data1$value.topik_20_1 <- lag(data1$value.topik_20,1)
  data1$value.topik_20_2 <- lag(data1$value.topik_20,2)
  data1$value.topik_20_3 <- lag(data1$value.topik_20,3)
  data1$value.topik_21_1 <- lag(data1$value.topik_21,1)
  data1$value.topik_21_2 <- lag(data1$value.topik_21,2)
  data1$value.topik_21_3 <- lag(data1$value.topik_21,3)
  data1$value.topik_22_1 <- lag(data1$value.topik_22,1)
  data1$value.topik_22_2 <- lag(data1$value.topik_22,2)
  data1$value.topik_22_3 <- lag(data1$value.topik_22,3)
  data1$value.topik_23_1 <- lag(data1$value.topik_23,1)
  data1$value.topik_23_2 <- lag(data1$value.topik_23,2)
  data1$value.topik_23_3 <- lag(data1$value.topik_23,3)
  data1$value.topik_24_1 <- lag(data1$value.topik_24,1)
  data1$value.topik_24_2 <- lag(data1$value.topik_24,2)
  data1$value.topik_24_3 <- lag(data1$value.topik_24,3)
  data1$value.topik_25_1 <- lag(data1$value.topik_25,1)
  data1$value.topik_25_2 <- lag(data1$value.topik_25,2)
  data1$value.topik_25_3 <- lag(data1$value.topik_25,3)
  #### удалим аномальные доходности####
  N <- which(abs(data_comp[,32])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  f <- c(f,length(N))
  
  #лассо
  mod <- glmnet(data1[-c(1:4),],data_comp[-c(1:4),32], 
                alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path <- as.data.frame(glmnetPath(mod)$leave)
  a <- max(glmnetPath(mod)$leave$ord)-2
  b <- max(glmnetPath(mod)$leave$ord)
  vec <- as.data.frame(a:b)
  names(vec)[1] <- 'ord'
  
  #отберем значимые топики
  path <- left_join(vec,path,by='ord')
  top <- c(path$var)
  
  #отберем топики
  reg <- data1 %>% dplyr::select(top)
  
  #лаги доходности
  data_comp$y_lag_1 <- lag(data_comp[,32], 1)
  data_comp$y_lag_2 <- lag(data_comp[,32], 2)
  data_comp$y_lag_3 <- lag(data_comp[,32], 3)
  data_y_lags <- as.data.frame(data_comp[-c(1:4),c(32:35)])
  names(data_y_lags)[1] <- 'yield'
  
  #по лагам
  set.seed(789)
  mod1 <- train(yield~.,
                data=data_y_lags, method = "lm",
                trControl = trainControl(method = 'repeatedcv',
                                         number = 10, repeats = 50))
  
  #достанем значимые лаги
  p_val <- as.data.frame(summary(mod1)$coefficients[-1,4]) 
  names(p_val)[1] <- 'pvalue'
  p_val <- p_val %>% filter(pvalue<0.1)
  lags <- c(rownames(p_val))
  lags <- data_comp %>% dplyr::select(lags)
  
  #сделаем датасет со значимым лагом доходности и значимыми топиками
  all <- as.data.frame(cbind(data_comp[,32], lags,reg))
  names(all)[1] <- 'yield'
  all <- all[-c(1:4),]
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  mod2 <- caret::train(yield ~ .,data=all, 
                       method = "lm",
                       trControl = trainControl(method = 'repeatedcv',
                                                number = 10, repeats = 50))
  summary(mod2)
  
  #достанем значимые лаги
  table_full_AR <- as.data.frame(summary(mod2)$coefficients)
  names(table_full_AR)[4] <- 'P-value'
  g <- grep('value.topik', rownames(table_full_AR))
  table <- as.data.frame(table_full_AR[g,])
  table_full_AR <- table %>% dplyr::filter(table[,4] < 0.1)
  table_full_AR_2 <- as.data.frame(rownames(table_full_AR))
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR else cbind(table_full_AR_2,table_full_AR)
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR_1 else table_full_AR_1 %>% mutate(ticker=list_comp[i])
  data11 <- rbind(data11,table_full_AR_1)
  t <- t.test(mod1$resample$Rsquared,mod2$resample$Rsquared)
  
  r2_no_news <- mean(mod1$resample$Rsquared)
  r2_with_news <- mean(mod2$resample$Rsquared)
  delta1 <- r2_with_news - r2_no_news
  
  adj_r2_no_news <- summary(mod1)$adj.r.squared
  adj_r2_with_news <- summary(mod2)$adj.r.squared
  delta2 <- adj_r2_with_news - adj_r2_no_news
  
  data13 <- rbind(data13,c(list_comp[i], t$statistic, 
                           r2_no_news, r2_with_news,
                           delta1,
                           adj_r2_no_news,adj_r2_with_news,
                           delta2))
}


View(data11)
View(data13)

data13 <- as.data.frame(data13)
data11 <- as.data.frame(data11)
View(data_topics)
write_xlsx(data11,'Топики_1_апреля.xlsx')
write_xlsx(data13,'AR_1_апреля.xlsx')

#data_10_50_ar13 <- as.data.frame(data13)
#data10_50_ar14 <- data_10_50_ar13 %>% mutate(t=as.numeric(t)) %>% filter(t < (-1.64))
#View(data10_50_ar14)
#write_xlsx(data_10_50_ar13,'AR_10_50_repCV1.xlsx')

#write_xlsx(data_5_10_ar,'final_AR_5_10_repCV.xlsx')

#View(data11)
#data6 <- as.data.frame(data6)
#library(writexl)
#write_xlsx(data11, 'normal_topics_arima_telegram_direction_100_CV.xlsx')

#### Список компаний ДЛЯ 4 ПЕРИОДОВ ####
list_comp <- c("ABIO",
               "ABRD",
               "AFKS",
               "AFLT",
               "AGRODR",
               "AKRN",
               "ALRS",
               "AMEZ",
               "APTK",
               "AQUA",
               "ARSA",
               "ASSB",
               "AVAN",
               "BANE",
               "BELU",
               "BLNG",
               "BRZL",
               "BSPB",
               "CBOM",
               "CHGZ",
               "CHKZ",
               "CHMF",
               "CHMK",
               "CIANDR",
               "CNTL",
               "DIOD",
               "DSKY",
               "DVEC",
               "DZRD",
               "EELT",
               "ELFV",
               "ELTZ",
               "ENPG",
               "ETLNDR",
               "FEES",
               "FESH",
               "FIVEDR",
               "FIXPDR",
               "FLOT",
               "FORTUM",
               "GAZA",
               "GAZP",
               "GCHE",
               "GEMA",
               "GEMCDR",
               "GLTRDR",
               "GMKN",
               "GTRK",
               "HHRUDR",
               "HMSGDR",
               "HYDR",
               "IGST",
               "INGR",
               "IRAO",
               "IRKT",
               "JNOS",
               "KAZT",
               "KBSB",
               "KCHE",
               "KGKC",
               "KLSB",
               "KMAZ",
               "KMEZ",
               "KOGK",
               "KRKN",
               "KROT",
               "KRSB",
               "KTSB",
               "KUBE",
               "KUZB",
               "KZOS",
               "LENT",
               "LIFE",
               "LKOH",
               "LNZL",
               "LPSB",
               "LSNG",
               "LSRG",
               "LVHK",
               "MAGE",
               "MAGN",
               "MDMGDR",
               "MERF",
               "MFGS",
               "MGNT",
               "MGTS",
               "MISB",
               "MOEX",
               "MRKC",
               "MRKK",
               "MRKP",
               "MRKS",
               "MRKU",
               "MRKV",
               "MRKY",
               "MRKZ",
               "MRSB",
               "MSNG",
               "MSRS",
               "MSTT",
               "MTLR",
               "MTSS",
               "MVID",
               "NAUK",
               "NFAZ",
               "NKHP",
               "NKNC",
               "NKSH",
               "NLMK",
               "NMTP",
               "NNSB",
               "NSVZ",
               "NVTK",
               "ODVA",
               "OGKB",
               "OKEYDR",
               "OZONDR",
               "PAZA",
               "PHOR",
               "PIKK",
               "PLZL",
               "PMSB",
               "POLY",
               "POSI",
               "PRFN",
               "PRMB",
               "QIWIDR",
               "RASP",
               "RBCM",
               "RDRB",
               "RENI",
               "RGSS",
               "RKKE",
               "RNFT",
               "ROLO",
               "ROSB",
               "ROSN",
               "ROST",
               "RTGZ",
               "RTKM",
               "RTSB",
               "RUAL",
               "RUGR",
               "RUSI",
               "RZSB",
               "SAGO",
               "SARE",
               "SBER",
               "SELG",
               "SFIN",
               "SGZH",
               "SIBN",
               "SLEN",
               "SMLT",
               "SNGS",
               "SPBE",
               "STSB",
               "SVET",
               "TASB",
               "TATN",
               "TCSGDR",
               "TGKA",
               "TGKB",
               "TGKN",
               "TNSE",
               "TORS",
               "TRMK",
               "TTLK",
               "TUZA",
               "UKUZ",
               "UNAC",
               "UNKL",
               "UPRO",
               "URKZ",
               "USBN",
               "UTAR",
               "UWGN",
               "VEON_RX",
               "VGSB",
               "VJGZ",
               "VKCO",
               "VLHZ",
               "VRSB",
               "VSMO",
               "VSYD",
               "VTBR",
               "WTCM",
               "YAKG",
               "YKEN",
               "YNDX",
               "YRSB",
               "ZILL",
               "ZVEZ")



#### новые данные 
#пакеты
library(dplyr)
library(glmnet)
library(solzy)
library(caret)
library(MASS)
library(randomForest)
library(reshape)   
library(readr)
library(readxl)

setwd("/Users/macbook/Desktop/Диплом/Доходность с топиками телеграм 4 периода")
file.list <- list.files(pattern='*.xlsx')
df.list <- lapply(file.list, as.data.frame(read_excel))

View(df.list)


#### загрузим для 4 периодов ####
library(reshape)   
library(readr)
library(readxl)
#обратимся к папке и достанем оттуда файлы 
setwd("/Users/macbook/Desktop/Диплом/Доходность с топиками телеграм 4 периода")
file.list <- list.files(pattern='*.xlsx')
df.list <- lapply(file.list, as.data.frame(read_excel))
View(df.list)

#### AR для 4 периодов ####
f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
data17 <- NULL
data11 <- NULL
#цикл для AR процесса
for(i in 1:193) {
  cat("Итерация номер", i, "\n")
  #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,c(1:11, 13:17,19,21:22,24:25,27:36)]
  names(data_comp)
  data_comp <- data_comp[order(as.Date(data_comp$value.data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$value.data))
  data_comp$value.data <- as.Date(data_comp$value.data)
  data_comp[,c(2:5, 12:31)] <- data_comp[,c(2:5, 12:31)] %>% mutate_all(as.numeric)
  data_comp[-1,32] <- diff(as.numeric(data_comp[,2]))/na.omit(dplyr::lag(data_comp$value.price, 1))
  names(data_comp)[8] <- 'date'
  data1 <- data_comp[,c(12:31)]
  names(data1)
  #### три лага топиков ####
  data1$value.topik_2_1 <- lag(data1$value.topik_2,1)
  data1$value.topik_2_2 <- lag(data1$value.topik_2,2)
  data1$value.topik_2_3 <- lag(data1$value.topik_2,3)
  data1$value.topik_3_1 <- lag(data1$value.topik_3,1)
  data1$value.topik_3_2 <- lag(data1$value.topik_3,2)
  data1$value.topik_3_3 <- lag(data1$value.topik_3,3)
  data1$value.topik_4_1 <- lag(data1$value.topik_4,1)
  data1$value.topik_4_2 <- lag(data1$value.topik_4,2)
  data1$value.topik_4_3 <- lag(data1$value.topik_4,3)
  data1$value.topik_5_1 <- lag(data1$value.topik_5,1)
  data1$value.topik_5_2 <- lag(data1$value.topik_5,2)
  data1$value.topik_5_3 <- lag(data1$value.topik_5,3)
  data1$value.topik_6_1 <- lag(data1$value.topik_6,1)
  data1$value.topik_6_2 <- lag(data1$value.topik_6,2)
  data1$value.topik_6_3 <- lag(data1$value.topik_6,3)
  data1$value.topik_8_1 <- lag(data1$value.topik_8,1)
  data1$value.topik_8_2 <- lag(data1$value.topik_8,2)
  data1$value.topik_8_3 <- lag(data1$value.topik_8,3)
  data1$value.topik_10_1 <- lag(data1$value.topik_10,1)
  data1$value.topik_10_2 <- lag(data1$value.topik_10,2)
  data1$value.topik_10_3 <- lag(data1$value.topik_10,3)
  data1$value.topik_11_1 <- lag(data1$value.topik_11,1)
  data1$value.topik_11_2 <- lag(data1$value.topik_11,2)
  data1$value.topik_11_3 <- lag(data1$value.topik_11,3)
  data1$value.topik_13_1 <- lag(data1$value.topik_13,1)
  data1$value.topik_13_2 <- lag(data1$value.topik_13,2)
  data1$value.topik_13_3 <- lag(data1$value.topik_13,3)
  data1$value.topik_14_1 <- lag(data1$value.topik_14,1)
  data1$value.topik_14_2 <- lag(data1$value.topik_14,2)
  data1$value.topik_14_3 <- lag(data1$value.topik_14,3)
  data1$value.topik_16_1 <- lag(data1$value.topik_16,1)
  data1$value.topik_16_2 <- lag(data1$value.topik_16,2)
  data1$value.topik_16_3 <- lag(data1$value.topik_16,3)
  data1$value.topik_17_1 <- lag(data1$value.topik_17,1)
  data1$value.topik_17_2 <- lag(data1$value.topik_17,2)
  data1$value.topik_17_3 <- lag(data1$value.topik_17,3)
  data1$value.topik_18_1 <- lag(data1$value.topik_18,1)
  data1$value.topik_18_2 <- lag(data1$value.topik_18,2)
  data1$value.topik_18_3 <- lag(data1$value.topik_18,3)
  data1$value.topik_19_1 <- lag(data1$value.topik_19,1)
  data1$value.topik_19_2 <- lag(data1$value.topik_19,2)
  data1$value.topik_19_3 <- lag(data1$value.topik_19,3)
  data1$value.topik_20_1 <- lag(data1$value.topik_20,1)
  data1$value.topik_20_2 <- lag(data1$value.topik_20,2)
  data1$value.topik_20_3 <- lag(data1$value.topik_20,3)
  data1$value.topik_21_1 <- lag(data1$value.topik_21,1)
  data1$value.topik_21_2 <- lag(data1$value.topik_21,2)
  data1$value.topik_21_3 <- lag(data1$value.topik_21,3)
  data1$value.topik_22_1 <- lag(data1$value.topik_22,1)
  data1$value.topik_22_2 <- lag(data1$value.topik_22,2)
  data1$value.topik_22_3 <- lag(data1$value.topik_22,3)
  data1$value.topik_23_1 <- lag(data1$value.topik_23,1)
  data1$value.topik_23_2 <- lag(data1$value.topik_23,2)
  data1$value.topik_23_3 <- lag(data1$value.topik_23,3)
  data1$value.topik_24_1 <- lag(data1$value.topik_24,1)
  data1$value.topik_24_2 <- lag(data1$value.topik_24,2)
  data1$value.topik_24_3 <- lag(data1$value.topik_24,3)
  data1$value.topik_25_1 <- lag(data1$value.topik_25,1)
  data1$value.topik_25_2 <- lag(data1$value.topik_25,2)
  data1$value.topik_25_3 <- lag(data1$value.topik_25,3)
  
  #### три лага доходностей ####
  data_comp$y_lag_1 <- lag(data_comp[,32], 1)
  data_comp$y_lag_2 <- lag(data_comp[,32], 2)
  data_comp$y_lag_3 <- lag(data_comp[,32], 3)
  data_y_lags <- as.data.frame(data_comp[,c(32:35)])
  names(data_y_lags)[1] <- 'yield'
  
  #####
  N <- which(abs(data_comp[,32])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  data_y_lags <- if(length(N)>0) data_y_lags[-N,] else data_y_lags
  f <- c(f,length(N))
  
  ##### разделим на 4 периода  ####
  data1 <- data1[-c(1:4),]
  data_comp <- data_comp[-c(1:4),]
  data_y_lags <- data_y_lags[-c(1:4),]
  
  dim_of_data <- as.data.frame(t(as.data.frame(dim(data_comp))))
  names(dim_of_data)[1] <- 'number'
  for_filter <- as.numeric(dim_of_data$number/4)
  
  data_comp1 <- data_comp[c(1:for_filter),]
  data_comp2 <- data_comp[c((for_filter+1):(2*for_filter)),]
  data_comp3 <- data_comp[c((2*for_filter+1):(3*for_filter)),]
  data_comp4 <- data_comp[c((3*for_filter+1):(4*for_filter)),]
  
  data1_1 <- data1[c(1:for_filter),]
  data1_2 <- data1[c((for_filter+1):(2*for_filter)),]
  data1_3 <- data1[c((2*for_filter+1):(3*for_filter)),]
  data1_4 <- data1[c((3*for_filter+1):(4*for_filter)),]
  
  data_y_lags1 <- data_y_lags[c(1:for_filter),]
  data_y_lags2 <- data_y_lags[c((for_filter+1):(2*for_filter)),]
  data_y_lags3 <- data_y_lags[c((2*for_filter+1):(3*for_filter)),]
  data_y_lags4 <- data_y_lags[c((3*for_filter+1):(4*for_filter)),]
  
  #### лассо для 1 периода ####
  mod1 <- glmnet(data1_1,data_comp1[,32], 
                 alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path1 <- as.data.frame(glmnetPath(mod1)$leave)
  a1 <- max(glmnetPath(mod1)$leave$ord)-2
  b1 <- max(glmnetPath(mod1)$leave$ord)
  vec1 <- as.data.frame(a1:b1)
  names(vec1)[1] <- 'ord'
  
  #отберем значимые топики
  path1 <- left_join(vec1,path1,by='ord')
  top1 <- c(path1$var)
  
  #отберем топики
  reg1 <- data1_1 %>% dplyr::select(top1)
  
  
  #### лассо для 2 периода ####
  mod2 <- glmnet(data1_2,data_comp2[,32], 
                 alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path2 <- as.data.frame(glmnetPath(mod2)$leave)
  a2 <- max(glmnetPath(mod2)$leave$ord)-2
  b2 <- max(glmnetPath(mod2)$leave$ord)
  vec2 <- as.data.frame(a2:b2)
  names(vec2)[1] <- 'ord'
  
  #отберем значимые топики
  path2 <- left_join(vec2,path2,by='ord')
  top2 <- c(path2$var)
  
  #отберем топики
  reg2 <- data1_2 %>% dplyr::select(top2)
  
  
  #### лассо для 3 периода ####
  mod3 <- glmnet(data1_3,data_comp3[,32], 
                 alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path3 <- as.data.frame(glmnetPath(mod3)$leave)
  a3 <- max(glmnetPath(mod3)$leave$ord)-2
  b3 <- max(glmnetPath(mod3)$leave$ord)
  vec3 <- as.data.frame(a3:b3)
  names(vec3)[1] <- 'ord'
  
  #отберем значимые топики
  path3 <- left_join(vec3,path3,by='ord')
  top3 <- c(path3$var)
  
  #отберем топики
  reg3 <- data1_3 %>% dplyr::select(top3)
  
  
  #### лассо для 4 периода ####
  mod4 <- glmnet(data1_4,data_comp4[,32], 
                 alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path4 <- as.data.frame(glmnetPath(mod4)$leave)
  a4 <- max(glmnetPath(mod4)$leave$ord)-2
  b4 <- max(glmnetPath(mod4)$leave$ord)
  vec4 <- as.data.frame(a4:b4)
  names(vec4)[1] <- 'ord'
  
  #отберем значимые топики
  path4 <- left_join(vec4,path4,by='ord')
  top4 <- c(path4$var)
  
  #отберем топики
  reg4 <- data1_4 %>% dplyr::select(top4)
  
  
  #1 период
  #только по топикам
  set.seed(789)
  mod1_1 <- train(yield~.,
                  data=data_y_lags1, method = "lm",
                  trControl = trainControl(method = 'repeatedcv',
                                           number = 10, repeats = 50))
  
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  all_1 <- cbind(data_y_lags1, reg1)
  mod1_2 <- caret::train(yield ~ .,data=all_1, 
                         method = "lm",
                         trControl = trainControl(method = 'repeatedcv',
                                                  number = 10, repeats = 50))
  
  #2 период
  #только по топикам
  set.seed(789)
  mod2_1 <- train(yield~.,
                  data=data_y_lags2, method = "lm",
                  trControl = trainControl(method = 'repeatedcv',
                                           number = 10, repeats = 50))
  
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  all_2 <- cbind(data_y_lags2, reg2)
  mod2_2 <- caret::train(yield ~ .,data=all_2, 
                         method = "lm",
                         trControl = trainControl(method = 'repeatedcv',
                                                  number = 10, repeats = 50))
  #3 период
  #только по топикам
  set.seed(789)
  mo3_1 <- train(yield~.,
                 data=data_y_lags3, method = "lm",
                 trControl = trainControl(method = 'repeatedcv',
                                          number = 10, repeats = 50))
  
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  all_3 <- cbind(data_y_lags3, reg3)
  mod3_2 <- caret::train(yield ~ .,data=all_3, 
                         method = "lm",
                         trControl = trainControl(method = 'repeatedcv',
                                                  number = 10, repeats = 50))
  
  #4 период
  #только по топикам
  set.seed(789)
  mo4_1 <- train(yield~.,
                 data=data_y_lags4, method = "lm",
                 trControl = trainControl(method = 'repeatedcv',
                                          number = 10, repeats = 50))
  
  
  #по значимым лагам доходности и топикам
  set.seed(789)
  all_4 <- cbind(data_y_lags4, reg4)
  mod4_2 <- caret::train(yield ~ .,data=all_4, 
                         method = "lm",
                         trControl = trainControl(method = 'repeatedcv',
                                                  number = 10, repeats = 50))
  
  
  t1 <- t.test(mod1_1$resample$Rsquared,mod1_2$resample$Rsquared)
  r2_no_news1 <-  summary(mod1_1)$adj.r.squared
  r2_with_news1 <- summary(mod1_2)$adj.r.squared
  delta1 <- r2_with_news1 - r2_no_news1
  
  t2 <- t.test(mod2_1$resample$Rsquared,mod2_2$resample$Rsquared)
  r2_no_news2 <-  summary(mod2_1)$adj.r.squared
  r2_with_news2 <- summary(mod2_2)$adj.r.squared
  delta2 <- r2_with_news2 - r2_no_news2
  
  t3 <- t.test(mo3_1$resample$Rsquared,mod3_2$resample$Rsquared)
  r2_no_news3 <-  summary(mo3_1)$adj.r.squared
  r2_with_news3 <- summary(mod3_2)$adj.r.squared
  delta3 <- r2_with_news3 - r2_no_news3
  
  t4 <- t.test(mo4_1$resample$Rsquared,mod4_2$resample$Rsquared)
  r2_no_news4 <-  summary(mo4_1)$adj.r.squared
  r2_with_news4 <- summary(mod4_2)$adj.r.squared
  delta4 <- r2_with_news4 - r2_no_news4
  
  period <- as.data.frame(for_filter)
  names(period)[1] <- 'Длина одного периода'
  
  #достанем значимые лаги 3 период
  table_full_AR_3 <- as.data.frame(summary(mod3_2)$coefficients)
  names(table_full_AR_3)[4] <- 'P-value'
  g3 <- grep('value.topik', rownames(table_full_AR_3))
  table3 <- as.data.frame(table_full_AR_3[g3,])
  table_full_AR_3 <- table3 %>% dplyr::filter(table3[,4] < 0.1)
  table_full_AR_3_2 <- as.data.frame(rownames(table_full_AR_3))
  table_full_AR_3_1 <- if(dim(table_full_AR_3)[1]==0) table_full_AR_3 else cbind(table_full_AR_3_2,table_full_AR_3)
  table_full_AR_3_1 <- if(dim(table_full_AR_3)[1]==0) table_full_AR_3 else table_full_AR_3_1 %>% mutate(ticker=list_comp[i])
  data17 <- rbind(data17,table_full_AR_3_1)
  
  #достанем значимые лаги 4 период
  table_full_AR <- as.data.frame(summary(mod4_2)$coefficients)
  names(table_full_AR)[4] <- 'P-value'
  g <- grep('value.topik', rownames(table_full_AR))
  table <- as.data.frame(table_full_AR[g,])
  table_full_AR <- table %>% dplyr::filter(table[,4] < 0.1)
  table_full_AR_2 <- as.data.frame(rownames(table_full_AR))
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR else cbind(table_full_AR_2,table_full_AR)
  table_full_AR_1 <- if(dim(table_full_AR)[1]==0) table_full_AR_1 else table_full_AR_1 %>% mutate(ticker=list_comp[i])
  data11 <- rbind(data11,table_full_AR_1)
  
  data9 <- rbind(data9,c(list_comp[i], 
                         t1$statistic,r2_no_news1,r2_with_news1,delta1,
                         t2$statistic,r2_no_news2,r2_with_news2,delta2,
                         t3$statistic,r2_no_news3,r2_with_news3,delta3,
                         t4$statistic,r2_no_news4,r2_with_news4,delta4,
                         period))}


View(data17)
View(data11)
View(data9)
data9 <- as.data.frame(data9)
datа11 <- as.data.frame(data11)
data17 <- as.data.frame(data17)

write_xlsx(data17, '1_апреля_третий_период_топики_AR.xlsx')
write_xlsx(data11, '1_апреля_четвертый_период_топики_AR.xlsx')
write_xlsx(data9, '1_апреля_4_periods_AR1.xlsx')

#### график ####
library(readxl)
AR_4_periods <- read_excel("~/Desktop/4_periods.xlsx")
#View(AR_4_periods)
library(ggplot2)
library(ComplexUpset)
columns <- colnames(AR_4_periods)[-1]
names(AR_4_periods)
upset(AR_4_periods, columns, 
      name = 'Влияние новостей на компанию по периодам',
      base_annotations = list("Количество компаний" = intersection_size(fill = "#153f65")), 
      set_sizes=upset_set_size() + ylab('Количество 1 в каждом периоде'),
      queries = list(upset_query(set = 'Первый период', fill = '#244e81'),
                     upset_query(set = 'Второй период', fill = '#19868f'), 
                     upset_query(set = 'Третий период', fill = '#bcd0e5'), 
                     upset_query(set = 'Четвертый период', fill = '#d3bb9c')), 
      matrix=(intersection_matrix(geom=geom_point(shape='circle filled', size=3)) + 
                scale_color_manual(values=c('Первый период'='#244e81', 'Второй период'='#19868f',
                                            'Третий период'='#bcd0e5', 'Четвертый период'='#d3bb9c'))))



##### AR процесс для всего периода по 200 компаниям ####
f <- NULL
data6 <- NULL
data8 <- NULL
data9 <- NULL
#цикл для AR процесса

for(i in 1:200) {
  cat("Итерация номер", i, "\n")
  #нормальный порядок дат в данных о компании
  data_comp <- data.frame(Reduce(rbind,df.list[i]))
  data_comp <- data_comp[,1:36]
  data_comp <- data_comp[order(as.Date(data_comp$value.data, format="%Y%m/%d/")),]
  data_comp <- data_comp %>% dplyr::arrange(mdy(data_comp$value.data))
  data_comp$value.data <- as.Date(data_comp$value.data)
  data_comp[,c(2:5, 12:36)] <- data_comp[,c(2:5, 12:36)] %>% mutate_all(as.numeric)
  data_comp[-1,37] <- diff(as.numeric(data_comp[,2]))/na.omit(dplyr::lag(data_comp$value.price, 1))
  names(data_comp)[8] <- 'date'
  data1 <- data_comp[,c(12:35)]
  
  #### три лага топиков ####
  data1$value.topik_1_1 <- lag(data1$value.topik_1,1)
  data1$value.topik_1_2 <- lag(data1$value.topik_1,2)
  data1$value.topik_1_3 <- lag(data1$value.topik_1,3)
  data1$value.topik_2_1 <- lag(data1$value.topik_2,1)
  data1$value.topik_2_2 <- lag(data1$value.topik_2,2)
  data1$value.topik_2_3 <- lag(data1$value.topik_2,3)
  data1$value.topik_3_1 <- lag(data1$value.topik_3,1)
  data1$value.topik_3_2 <- lag(data1$value.topik_3,2)
  data1$value.topik_3_3 <- lag(data1$value.topik_3,3)
  data1$value.topik_4_1 <- lag(data1$value.topik_4,1)
  data1$value.topik_4_2 <- lag(data1$value.topik_4,2)
  data1$value.topik_4_3 <- lag(data1$value.topik_4,3)
  data1$value.topik_5_1 <- lag(data1$value.topik_5,1)
  data1$value.topik_5_2 <- lag(data1$value.topik_5,2)
  data1$value.topik_5_3 <- lag(data1$value.topik_5,3)
  data1$value.topik_6_1 <- lag(data1$value.topik_6,1)
  data1$value.topik_6_2 <- lag(data1$value.topik_6,2)
  data1$value.topik_6_3 <- lag(data1$value.topik_6,3)
  data1$value.topik_7_1 <- lag(data1$value.topik_7,1)
  data1$value.topik_7_2 <- lag(data1$value.topik_7,2)
  data1$value.topik_7_3 <- lag(data1$value.topik_7,3)
  data1$value.topik_8_1 <- lag(data1$value.topik_8,1)
  data1$value.topik_8_2 <- lag(data1$value.topik_8,2)
  data1$value.topik_8_3 <- lag(data1$value.topik_8,3)
  data1$value.topik_9_1 <- lag(data1$value.topik_9,1)
  data1$value.topik_9_2 <- lag(data1$value.topik_9,2)
  data1$value.topik_9_3 <- lag(data1$value.topik_9,3)
  data1$value.topik_10_1 <- lag(data1$value.topik_10,1)
  data1$value.topik_10_2 <- lag(data1$value.topik_10,2)
  data1$value.topik_10_3 <- lag(data1$value.topik_10,3)
  data1$value.topik_11_1 <- lag(data1$value.topik_11,1)
  data1$value.topik_11_2 <- lag(data1$value.topik_11,2)
  data1$value.topik_11_3 <- lag(data1$value.topik_11,3)
  data1$value.topik_12_1 <- lag(data1$value.topik_12,1)
  data1$value.topik_12_2 <- lag(data1$value.topik_12,2)
  data1$value.topik_12_3 <- lag(data1$value.topik_12,3)
  data1$value.topik_13_1 <- lag(data1$value.topik_13,1)
  data1$value.topik_13_2 <- lag(data1$value.topik_13,2)
  data1$value.topik_13_3 <- lag(data1$value.topik_13,3)
  data1$value.topik_14_1 <- lag(data1$value.topik_14,1)
  data1$value.topik_14_2 <- lag(data1$value.topik_14,2)
  data1$value.topik_14_3 <- lag(data1$value.topik_14,3)
  data1$value.topik_15_1 <- lag(data1$value.topik_15,1)
  data1$value.topik_15_2 <- lag(data1$value.topik_15,2)
  data1$value.topik_15_3 <- lag(data1$value.topik_15,3)
  data1$value.topik_16_1 <- lag(data1$value.topik_16,1)
  data1$value.topik_16_2 <- lag(data1$value.topik_16,2)
  data1$value.topik_16_3 <- lag(data1$value.topik_16,3)
  data1$value.topik_17_1 <- lag(data1$value.topik_17,1)
  data1$value.topik_17_2 <- lag(data1$value.topik_17,2)
  data1$value.topik_17_3 <- lag(data1$value.topik_17,3)
  data1$value.topik_18_1 <- lag(data1$value.topik_18,1)
  data1$value.topik_18_2 <- lag(data1$value.topik_18,2)
  data1$value.topik_18_3 <- lag(data1$value.topik_18,3)
  data1$value.topik_19_1 <- lag(data1$value.topik_19,1)
  data1$value.topik_19_2 <- lag(data1$value.topik_19,2)
  data1$value.topik_19_3 <- lag(data1$value.topik_19,3)
  data1$value.topik_20_1 <- lag(data1$value.topik_20,1)
  data1$value.topik_20_2 <- lag(data1$value.topik_20,2)
  data1$value.topik_20_3 <- lag(data1$value.topik_20,3)
  data1$value.topik_21_1 <- lag(data1$value.topik_21,1)
  data1$value.topik_21_2 <- lag(data1$value.topik_21,2)
  data1$value.topik_21_3 <- lag(data1$value.topik_21,3)
  data1$value.topik_22_1 <- lag(data1$value.topik_22,1)
  data1$value.topik_22_2 <- lag(data1$value.topik_22,2)
  data1$value.topik_22_3 <- lag(data1$value.topik_22,3)
  data1$value.topik_23_1 <- lag(data1$value.topik_23,1)
  data1$value.topik_23_2 <- lag(data1$value.topik_23,2)
  data1$value.topik_23_3 <- lag(data1$value.topik_23,3)
  data1$value.topik_24_1 <- lag(data1$value.topik_24,1)
  data1$value.topik_24_2 <- lag(data1$value.topik_24,2)
  data1$value.topik_24_3 <- lag(data1$value.topik_24,3)
  #### удалим аномальные доходности####
  N <- which(abs(data_comp[,37])>0.2) 
  data1 <- if(length(N)>0) data1[-N,] else data1
  data_comp <- if(length(N)>0) data_comp[-N,] else data_comp
  f <- c(f,length(N))
  
  #лассо
  mod <- glmnet(data1[-c(1:4),],data_comp[-c(1:4),37], 
                alpha = 1, lambda = seq(from=0, to = 0.1,by=0.0001))
  
  #внутри лассо
  path <- as.data.frame(glmnetPath(mod)$leave)
  a <- max(glmnetPath(mod)$leave$ord)-2
  b <- max(glmnetPath(mod)$leave$ord)
  vec <- as.data.frame(a:b)
  names(vec)[1] <- 'ord'
  
  #отберем значимые топики
  path <- left_join(vec,path,by='ord')
  top <- c(path$var)
  
  #отберем топики
  reg <- data1 %>% dplyr::select(top)
  
  #лаги доходности
  data_comp$y_lag_1 <- lag(data_comp[,37], 1)
  data_comp$y_lag_2 <- lag(data_comp[,37], 2)
  data_comp$y_lag_3 <- lag(data_comp[,37], 3)
  data_y_lags <- as.data.frame(data_comp[-c(1:4),c(37:40)])
  names(data_y_lags)[1] <- 'yield'
  #AR только по лагам доходности
  set.seed(789)
  mod1 <- train(yield~.,
                data=data_y_lags, method = "lm",
                trControl = trainControl(method = 'repeatedcv',
                                         number = 10, repeats = 50))
  
  #сделаем датасет со значимым лагом доходности и значимыми топиками
  all <- as.data.frame(cbind(data_y_lags,reg[-c(1:4),]))
  names(all)[1] <- 'yield'
  all <- na.omit(all)
  
  #по значимым лагам доходности и топикам
  #10-кратная кросс-валидация
  set.seed(789)
  mod2 <- caret::train(yield ~ .,data=all, 
                       method = "lm",
                       trControl = trainControl(method = 'repeatedcv',
                                                number = 10, repeats = 50))
  
  t <- t.test(mod1$resample$Rsquared,mod2$resample$Rsquared)
  r2_no_news <- mean(mod1$resample$Rsquared)
  r2_with_news <- mean(mod2$resample$Rsquared)
  delta1 <- r2_with_news - r2_no_news
  
  adj_r2_no_news <- summary(mod1)$adj.r.squared
  adj_r2_with_news <- summary(mod2)$adj.r.squared
  delta2 <- adj_r2_with_news - adj_r2_no_news
  
  data9 <- rbind(data9,c(list_comp[i], t$statistic, 
                         r2_no_news, r2_with_news,
                         delta1,
                         adj_r2_no_news,adj_r2_with_news,
                         delta2))}

View(data9)
data9 <- as.data.frame(data9)
View(data9)

#10-кратная кросс-валидация с 50 повторениями
data_10_50_ar <- as.data.frame(data9)
write_xlsx(data_10_50_ar, 'AR_10_50_repCV.xlsx')
data10_50_ar1 <- data_10_50_ar %>% mutate(t=as.numeric(t)) %>% filter(t < (-1.64))
View(data10_50_ar1)
View(data_10_50_ar)

library(readxl)
new_stocks_sectors <- read_excel("~/Desktop/new_stocks_sectors.xlsx")
View(new_stocks_sectors)
names(data_10_50_ar)[1] <- 'ticker'
data_all_ar <- left_join(new_stocks_sectors,data_10_50_ar,by='ticker')
View(data_all_ar)

write_xlsx(data_all_ar,'proda.xlsx')


data_10_50_ar <- data_10_50_ar %>% mutate(V6=as.numeric(V6), V7=as.numeric(V7))
t.test(data_10_50_ar$V6,data_10_50_ar$V7)

#5 кратная кросс-валидация и 10 повторений
data_5_10_ar <- as.data.frame(data9)
data5_10_ar1 <- data_5_10_ar %>% mutate(t=as.numeric(t)) %>% filter(t < (-1.96))
View(data5_10_ar1)
write_xlsx(data_5_10_ar,'final_AR_5_10_repCV.xlsx')

#10 кратная кросс-валидация и 10 повторений
data_10_10_ar <- as.data.frame(data9)
data10_10_ar1 <- data_10_10_ar %>% mutate(t=as.numeric(t)) %>% filter(t < (-1.64))
View(data10_10_ar1)
View(data_10_10_ar)

write_xlsx(data_10_10_ar,'final_AR_10_10_repCV.xlsx')

#10-кратная с 5-кратным повторением 
data_10_5_ar <- as.data.frame(data9)
data10_5_ar1 <- data_10_5_ar %>% mutate(t=as.numeric(t)) %>% filter(t < (-1.64))
View(data10_5_ar1)



#10 и 10
data_1234 <- as.data.frame(data9)
data1234 <- data_1234 %>% mutate(t=as.numeric(t)) %>% filter(t < (-1.64))
View(data1234)
#data9$t <- as.numeric(data9$t)
#data10 <- data9 %>% filter(t<=-1.65)
#View(data10)
#View(data_comp)

library(writexl)
write_xlsx(data9,'AR_telegram_full_10_10.xlsx')

########### сравнение отраслей и регионов #############
pertest <- read_excel("/Users/macbook/Desktop/Диплом/Доходность с топиками телеграм 4 периода/4_апреля_итог.xlsx")
a <- c(pertest$for_filter)
b <- c(pertest$sector)
chisq.test(b,a)
d <- c(pertest$region)
chisq.test(d,a)
#mosaicplot(a)

pertest2 <- read_excel("/Users/macbook/Desktop/апрель/1_april_4_periods.xlsx")
pertest3 <- pertest2[,-1]
pertest3 <- mutate_all(pertest3, as.numeric)
pertest3 <- as.data.frame(pertest3)
View(pertest3)
names(pertest3) <- c('A','B','C','D')
chisq.test(pertest3$A, pertest3$B)
chisq.test(pertest3$A, pertest3$C)
chisq.test(pertest3$A, pertest3$D)
chisq.test(pertest3$B, pertest3$C)
chisq.test(pertest3$B, pertest3$D)
chisq.test(pertest3$C, pertest3$D)

#85 87 124 115
prop.test(x = c(85, 87), n = c(193, 193))
prop.test(x = c(85, 124), n = c(193, 193))
prop.test(x = c(85, 115), n = c(193, 193))
prop.test(x = c(87, 124), n = c(193, 193))
prop.test(x = c(87, 115), n = c(193, 193))
prop.test(x = c(115, 124), n = c(193, 193))

prop.test(x = c(85, 87), n = c(193, 193))
